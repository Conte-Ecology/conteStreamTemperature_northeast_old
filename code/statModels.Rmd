Overall approach: 
1) Define bps and slopes by segment for each site/year combo in temperatureSegmentedBreakPointAnalysis.Rmd. 
2) Model slopes for each segment (2=sp-sum, 3=sum-autumn) as a function of airTemp and fixed covariates. This gets predicted water temp as a function of airTemp and covariates, but does not identify bps.
3) Model bps as a fucntion of covariates including swe for bp1.
4) Predict water temp as function of airTemp and covariates between bps for each prediction site
5) Summarize data for slopes btw bps

Note: run temperatureSegmentedBreakPointAnalysis.Rmd before running this script

```{r load libs}
rm(list=ls())

library(ggplot2)
library(relaimpo)
library(lme4)
library(DataCombine) # for the slide function
library(plyr)
library(reshape)
library(ggmap)
library(foreign)
library(maptools)
library(gridExtra)
library(zoo)

#setwd('/Users/Dan/Documents/Research/Stream_Climate_Change/temperatureProject/')
setwd('C:/KPONEIL/GitHub/projects/temperatureProject/')


baseDir <- 'C:/KPONEIL/GitHub/projects/temperatureProject/'
#baseDir <- '/Users/Dan/Documents/Research/Stream_Climate_Change/temperatureProject/'

dataInDir <- paste0(baseDir, 'dataIn/')
dataOutDir <- paste0(baseDir, 'dataOut/')
graphsDir <- paste0(baseDir, 'graphs/')

source(paste0(baseDir, 'code/functions/temperatureModelingFunctions.R'))

```

Which agencies do you want to pull data from?
```{r Define data sources and other options}

#If removeSelectSites = TRUE, then the file with the list of sites needs to be specified.
removeSelectSites <- T
sitesToRemove <- paste0(baseDir, 'dataIn/sitesToRemoveAllNE.csv')

#Do you want all of the plots made?
makePlots <- T

#Use validation?
validate=T
  
#If validating:
  # Choose fraction of total # of sites:
  validateFrac <- 0.1

  #Do you want to create bias maps? (Internet connection required)
  createBiasMaps <- F

#Data source agencies?
CTDEP  <- T
MAFW   <- T
MAUSGS <- T
NHFG   <- T
NHDES  <- T
USFS   <- T
VTFWS  <- T
MEDMR  <- F
MTUSGSYellowstone <- F
MTUSGSGlacier <- F

#global vars
dpiIn <- 400

```

```{r load data}

#Set up data list.
sourceChoice <- list( CTDEP,   MAFW,   MAUSGS,   NHFG,   NHDES,   MEDMR,   USFS,   VTFWS,    MTUSGSYellowstone,   MTUSGSGlacier )
sourceNames  <- c   ('CTDEP', 'MAFW', 'MAUSGS', 'NHFG', 'NHDES', 'MEDMR', 'USFS', 'VTFWS',  'MTUSGSYellowstone', 'MTUSGSGlacier')

dataSource <- sourceNames[sourceChoice == T]

#sourceList <- paste0(paste0(dataSource, collapse = '_'))
sourceList <- paste0(paste0(dataSource, collapse = '_'), '_2014-04-24')

#Load "et" for the agencies:
load(paste0(dataOutDir, sourceList,  '/et_', sourceList, '.RData'))

#Pull duplicate columns so they doesn't get doubled up
et <- et[,-which(names(et) %in% c('Latitude', 'Longitude', 'StreamOrder', 'HUC_4', 'HUC_8.x', 'HUC_8.y', 'HUC_12', 'agency'))]

#Load in covariate data to merge into slopes df [no day data]
for ( i in 1:length(dataSource)){

  #Load covariate data to be merged into slopes df [no day data]
  load(paste0(dataInDir, dataSource[i], '/covariateData_', dataSource[i], '.RData')) #Fixed over time
  covariateData$agency <- paste(dataSource[i])
  if ( i == 1) {covs <- covariateData} else (covs <- rbind(covs, covariateData))
  
  #Load daymet climate data to be merged into et:
  load(paste0(dataInDir, dataSource[i], '/streamTempSitesObservedClimateData_', dataSource[i], '.RData')) 
  
  #Pull out the columns needed:
  masterData <- masterData[, c('site', 'year', 'dOY', 'date', 'dayl', 'srad', 'swe', 'tmax', 'tmin', 'vp', 'prcp')]
  if ( i == 1) {newDay <- masterData} else ( newDay <- rbind(newDay, masterData) )
}

masterData    <- newDay
covariateData <- covs


#Merge climate data into main dataframe:
et <- merge(et, masterData, by = c('site', 'date', 'year', 'dOY'), all.x=T, sort = F )

et$flow <- NA
et$tAirMin <- et$tmin; et$tAirMax <- et$tmax

#Overwrite NaNs with NAs:
covariateData <- replace(covariateData, is.na(covariateData), NA)

#Make site a character string so the "merge" function works:
covariateData$site <- as.character(covariateData$site)

#=================================================================================================================
#Scale the variables used in the model. Some get log-scaled depending on their distribution over the sites:
#=================================================================================================================
# Standard scaling:
covariateData$LatitudeS              <- (covariateData$Latitude        - mean(covariateData$Latitude       , na.rm=T)) / sd(covariateData$Latitude       , na.rm=T)
covariateData$LongitudeS             <- (covariateData$Longitude       - mean(covariateData$Longitude      , na.rm=T)) / sd(covariateData$Longitude      , na.rm=T)
covariateData$ForestS                <- (covariateData$Forest          - mean(covariateData$Forest         , na.rm=T)) / sd(covariateData$Forest         , na.rm=T)
covariateData$BasinElevationMS       <- (covariateData$BasinElevationM - mean(covariateData$BasinElevationM, na.rm=T)) / sd(covariateData$BasinElevationM, na.rm=T)
covariateData$ReachSlopePCNTS        <- (covariateData$ReachSlopePCNT  - mean(covariateData$ReachSlopePCNT , na.rm=T)) / sd(covariateData$ReachSlopePCNT , na.rm=T)
covariateData$WetlandOrWaterS        <- (covariateData$WetlandOrWater  - mean(covariateData$WetlandOrWater , na.rm=T)) / sd(covariateData$WetlandOrWater , na.rm=T)

# Log scaling:
covariateData$AgricultureLS          <- (log(covariateData$Agriculture          + 0.001) - mean(log(covariateData$Agriculture          + 0.001), na.rm=T)) / sd(log(covariateData$Agriculture          + 0.001), na.rm=T)
covariateData$TotDASqKMLS            <- (log(covariateData$TotDASqKM            + 0.001) - mean(log(covariateData$TotDASqKM            + 0.001), na.rm=T)) / sd(log(covariateData$TotDASqKM            + 0.001), na.rm=T)
covariateData$SurficialCoarseCLS     <- (log(covariateData$SurficialCoarseC     + 1    ) - mean(log(covariateData$SurficialCoarseC     + 1    ), na.rm=T)) / sd(log(covariateData$SurficialCoarseC     + 1    ), na.rm=T)
covariateData$ImpoundmentsOpenSqKMLS <- (log(covariateData$ImpoundmentsOpenSqKM + 1    ) - mean(log(covariateData$ImpoundmentsOpenSqKM + 1    ), na.rm=T)) / sd(log(covariateData$ImpoundmentsOpenSqKM + 1    ), na.rm=T)


et <- merge(et, covariateData, by = 'site', all.x=T, sort = F )

#====================================================================================================
#Remove slected site/years that have errors in breakpoint assignment (chosen via visual examination):
#====================================================================================================
if( removeSelectSites ) {
  
  removeSites <- read.csv(sitesToRemove)
  
  removeSites$site <- as.character(removeSites$site)
  
  et <-  et[!(et$site %in% removeSites$site & et$year %in% removeSites$year),]
}
#====================================================================================================

#Get BPs out of et
bp <- unique(et[,c('site','year','springBP','summerBP','fallBP')]  ) #, 'Latitude', 'Longitude'
bp <- bp[is.finite(bp$springBP) | is.finite(bp$summerBP) | is.finite(bp$fallBP),]
bp$site <- as.character(bp$site) #for merging
  
siteData <- merge( x = bp, y = covariateData, by = 'site', all.x=T )

# turn Inf to NA in bps
siteData[!is.finite(siteData$springBP),'springBP'] <- NA
siteData[!is.finite(siteData$summerBP),'summerBP'] <- NA
siteData[!is.finite(siteData$fallBP),'fallBP'] <- NA

# merge in count of days
obsBySiteYear <- ddply(et, .(site,year), summarize,count=length(!is.na(temp)))
siteData <- merge(x=siteData, y=obsBySiteYear, all.x=T)

```

```{r lag airTemp & prcp}
et <- et[order(et$count),] # just to make sure et is ordered for the slide function

# airTemp
et <- slide(et, Var = "airTemp", GroupVar = "site", slideBy = -1, NewVar='airTempLagged1')
et <- slide(et, Var = "airTemp", GroupVar = "site", slideBy = -2, NewVar='airTempLagged2')

# prcp
et <- slide(et, Var = "prcp", GroupVar = "site", slideBy = -1, NewVar='prcpLagged1')
et <- slide(et, Var = "prcp", GroupVar = "site", slideBy = -2, NewVar='prcpLagged2')
et <- slide(et, Var = "prcp", GroupVar = "site", slideBy = -3, NewVar='prcpLagged3')

# 5-day mean of prcp
siteYearCombos <- unique(et[,c('site','year')])

et$prcp5Day <- NA

window <- 5
for (i in 1:nrow(siteYearCombos)){

  print(c(i,as.character(siteYearCombos$site[i]),siteYearCombos$year[i],i/nrow(siteYearCombos)))
  
  currSite <- which(et$site == as.character(siteYearCombos$site[i]) & et$year == siteYearCombos$year[i] )

  #Need this so sites with very short records don't crash the loop.
  if(length(currSite) >= window){currMean <-  rollapply(et$prcp[currSite], width=window, fill=NA, mean, align = 'left')} else(currMean <- NA)
  
  et$prcp5Day[currSite] <- currMean
}


```

```{r check out data}
#pairs(~Latitude+Longitude+Forest+ Impervious+ Agriculture+ BasinElevationM+ ReachSlopePCNT+ TotDASqKM+ WetlandOrWater+ SurficialCoarseC,data=et)
#Latitude, Longitude, Forest, Impervious, Agriculture, BasinElevationM, ReachSlopePCT, TotDASqKM, WetlandOrWater, SurficialCoarseC

if(makePlots) {

  #Makes barcode looking plot of data records:
  #-------------------------------------------
  gTile <- 
  ggplot(siteData,aes(site,year,z=any(c(!is.na(springBP),!is.na(summerBP),!is.na(fallBP)))))+
    scale_x_discrete('Site')+
    scale_y_continuous('Year')+
    theme_bw(base_size=20) + 
      theme(axis.text.x = element_blank())+
    geom_tile()
  
  ggsave( file=paste0(graphsDir, sourceList, '/gTile.png'), plot=gTile, dpi=dpiIn , width=8,height=5, units='in' )
  
  #Colors by number of observations?
  #---------------------------------
  gTileHeat <- 
  ggplot(siteData,aes(site,year,z=count))+
    geom_tile(aes(fill=count))+
    scale_x_discrete('Site')+
    scale_y_continuous('Year')+ 
    theme(axis.text.x = element_blank())
  
  ggsave( file=paste0(graphsDir, sourceList,'/gTileHeat.png'), plot=gTileHeat, dpi=dpiIn , width=8,height=5, units='in' )

}

```

```{r scale daymet vars and split et into rising and falling segments}
# Don't use "scale" because it creates a data type with attributes that make is difficult to use predict

logTimeSeries <- function(dataFrame, variableNames){
  
  for ( i in variableNames ){
    dataFrame[,paste0(i, 'L')] <- log(dataFrame[,i]  + 0.001)
  }
  return(dataFrame)
}


#List the variables you want logged:
logVariables <- c('swe', 'prcp', 'prcpLagged1', 'prcpLagged2', 'prcpLagged3', 'prcp5Day')

et <- logTimeSeries(et, logVariables)


# Split dataframe into segments:
et2 <- et[et$segment %in% 2,]
et3 <- et[et$segment %in% 3,]



scaleVariables <- c('swe', 'prcp')

scaleDaymetVariables <- function ( dataFrame, scaleVars){

  for ( i in scaleVars ){
    dataFrame[,paste0(i, 'S')]  <- (dataFrame[,i]  - mean(dataFrame[,i],na.rm=T) ) / sd(dataFrame[,i], na.rm=T )
  }
  
  return(dataFrame)
}






loggAndScaleDaymet <- function(dataFrame, logVars, scaleVars){
  
  for ( i in logVars ){
    dataFrame[,paste0(i, 'L')] <- log(dataFrame[,i]  + 0.001)
  }
  
  dataFrameSeg2 <- dataFrame[dataFrame$segment %in% 2,]
  dataFrameSeg3 <- dataFrame[dataFrame$segment %in% 3,]
  
  for ( j in scaleVars ){
    
    if( j %in% logVars ) {dataFrameSeg2[,paste0(j, 'LS')]  <- (dataFrameSeg2[,paste0(j, 'L')]  - mean(dataFrameSeg2[,paste0(j, 'L')],na.rm=T) ) / sd(dataFrameSeg2[,paste0(j, 'L')], na.rm=T )} 
      else (dataFrameSeg2[,paste0(j, 'S')]  <- (dataFrameSeg2[,j]  - mean(dataFrameSeg2[,j],na.rm=T) ) / sd(dataFrameSeg2[,j], na.rm=T ))
    
    if( j %in% logVars ) {dataFrameSeg3[,paste0(j, 'LS')]  <- (dataFrameSeg3[,paste0(j, 'L')]  - mean(dataFrameSeg3[,paste0(j, 'L')],na.rm=T) ) / sd(dataFrameSeg3[,paste0(j, 'L')], na.rm=T )} 
      else (dataFrameSeg3[,paste0(j, 'S')]  <- (dataFrameSeg3[,j]  - mean(dataFrameSeg3[,j],na.rm=T) ) / sd(dataFrameSeg3[,j], na.rm=T ))  
  }
 
  return(list(dataFrame, dataFrameSeg2, dataFrameSeg3))
}



#List the variables you want logged.
logVariables <- c('swe', 'prcp', 'prcpLagged1', 'prcpLagged2', 'prcpLagged3', 'prcp5Day')

#List the variables you want scaled. (If you want the logged ones scaled include them as well)
scaleVariables <- c('swe', 'dayl', 'srad', 'prcp', 'prcpLagged1', 'prcpLagged2', 'prcpLagged3', 'prcp5Day')


logScaledData <- logTimeSeries(et, logVariables, scaleVariables)

et  <- logScaledData[[1]]
et2 <- logScaledData[[2]]
et3 <- logScaledData[[3]]



#
#
# Can probably remove this stuff and just go with the function.
#
# Test it first!
#
#


# Scale daymet vaiables in segment 2:
et2$daylS  <- (et2$dayl  - mean(et2$dayl,na.rm=T) ) / sd(et2$dayl, na.rm=T )
et2$sradS  <- (et2$srad  - mean(et2$srad,na.rm=T) ) / sd(et2$srad, na.rm=T )
et2$sweLS  <- (et2$sweL  - mean(et2$sweL,na.rm=T) ) / sd(et2$sweL, na.rm=T )
et2$prcpLS <- (et2$prcpL - mean(et2$prcpL,na.rm=T)) / sd(et2$prcpL, na.rm=T)

# prcp test
#===========================================================================================
et2$prcpLS1 <- (et2$prcpL1 - mean(et2$prcpL1,na.rm=T)) / sd(et2$prcpL1, na.rm=T)
et2$prcpLS2 <- (et2$prcpL2 - mean(et2$prcpL2,na.rm=T)) / sd(et2$prcpL2, na.rm=T)
et2$prcpLS3 <- (et2$prcpL3 - mean(et2$prcpL3,na.rm=T)) / sd(et2$prcpL3, na.rm=T)
et2$prcp5DayLS <- (et2$prcp5DayL - mean(et2$prcp5DayL,na.rm=T)) / sd(et2$prcp5DayL, na.rm=T)
#===========================================================================================


# Scale daymet vaiables in segment 3:
et3$daylS  <- (et3$dayl  - mean(et3$dayl,na.rm=T) ) / sd(et3$dayl, na.rm=T )
et3$sradS  <- (et3$srad  - mean(et3$srad,na.rm=T) ) / sd(et3$srad, na.rm=T )
et3$sweLS  <- (et3$sweL  - mean(et3$sweL,na.rm=T) ) / sd(et3$sweL, na.rm=T ) #Make sure all values aren't zero if you want to use this in stats models:
et3$prcpLS <- (et3$prcpL - mean(et3$prcpL,na.rm=T)) / sd(et3$prcpL, na.rm=T)

# prcp test
#===========================================================================================
et3$prcpLS1 <- (et3$prcpL1 - mean(et3$prcpL1,na.rm=T)) / sd(et3$prcpL1, na.rm=T)
et3$prcpLS2 <- (et3$prcpL2 - mean(et3$prcpL2,na.rm=T)) / sd(et3$prcpL2, na.rm=T)
et3$prcpLS3 <- (et3$prcpL3 - mean(et3$prcpL3,na.rm=T)) / sd(et3$prcpL3, na.rm=T)
et3$prcp5DayLS <- (et3$prcp5DayL - mean(et3$prcp5DayL,na.rm=T)) / sd(et3$prcp5DayL, na.rm=T)
#===========================================================================================


```

Models for rising & falling segments
------------------------------------
When adding new models:
   1) Follow the structure of previously defined models and name it: "mX"
   2) Assign the description of the model and name it: "dX"
   3) Add new models to the model lists ("segModels" and "segModelsDescriptions")
  
```{r regression temp~airTemp+...}
#This section explores modeling temperature using fixed covariate data and time series of climate data.
# Models are created by segment (warming and cooling).

#=================================================================================================================
#                                         Define the Segment Models:
#=================================================================================================================

#Model 1:
d1 <- 'Air temp & lags only'
#---------------------------
m1 <- 'temp~airTemp+airTempLagged1+airTempLagged2'


#Model 2:

d2 <- 'All main effects'
#-----------------------
m2 <-'temp~airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS'


#Model 3:
d3 <- 'All two-way interactions'
#-------------------------------
m3 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS)^2'

#Model 4:
d4 <- 'All main effects (add prcp)'
#----------------------------------
m4 <- 'temp~airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS'

#Model 5:
d5 <- 'All two-way interactions (add prcp)'
#------------------------------------------
m5 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS)^2'

#Model 6:
d6 <- 'All two-way interactions (prcp Lag +1)'
#---------------------------------------------
m6 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS1)^2'

#Model 7:
d7 <- 'All two-way interactions (prcp Lag +2)'
#---------------------------------------------
m7 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS2)^2'


#Model 8:
d8 <- 'All two-way interactions (prcp Lag +3)'
#---------------------------------------------------------------------
m8 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS3)^2'


#Model 9:
d9 <- 'All two-way interactions (prcp 5-day mean)'
#-------------------------------------------------
m9 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcp5DayLS)^2'

#Model 10:
d10 <- 'All two-way interactions (prcp  + prcpLags 1, 2, & 3)'
#------------------------------------------------------
m10 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS + prcpLS1 + prcpLS2 + prcpLS3)^2'

#Model 11:
d11 <- 'All two-way interactions (prcp + prcp Lag +1)'
#---------------------------------------------
m11 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS + prcpLS1)^2'

#Model 12:
d12 <- 'All two-way interactions (prcp + prcp Lag +2)'
#---------------------------------------------
m12 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS  + prcpLS2)^2'

#Model 13:
d13 <- 'All two-way interactions (prcp + prcp Lag +3)'
#---------------------------------------------------------------------
m13 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS + prcpLS3)^2'

#Model 14:
d14 <- 'All two-way interactions (prcp + prcp 5-day mean)'
#-------------------------------------------------
m14 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS + prcp5DayLS)^2'

#Model 15:
d15 <- 'All two-way interactions (all prcp terms)'
#------------------------------------------------------
m15 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS  + prcpLS1 + prcpLS2 + prcpLS3 + prcp5DayLS)^2'

segModels             <- list(m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11, m12, m13, m14, m15)
segModelsDescriptions <- list(d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12, d13, d14, d15)

#=================================================================================================================
#                                                    Segment 2
#=================================================================================================================
#Validate for a random subset of sites:

randomSitesS2 <- sample(unique(et2$site), size = validateFrac*length(unique(et2$site)))
valData2    <- et2[et2$site %in% randomSitesS2,]

mS2  <- list()
vmS2 <- list()

#for (i in 1:length(segModels)){
for (i in 10:length(segModels)){  
  print(i/length(segModels))  
  
  mS2[[i]] <- lm(as.formula(segModels[[i]]), data=et2)
  
  if (validate){
    vmS2[[i]] <- validateModel( mS2[[i]], et2, valData2 )
    if(createBiasMaps) {makeBiasMap(vmS2[[i]])}
  }
}

#-------------------
#Compare the models:
#-------------------
if(validate){
  for ( i in 1:length(vmS2) ){   
      a <- data.frame(vmS2[[i]]$means, extractAIC(mS2[[i]])[1], extractAIC(mS2[[i]])[2], summary(mS2[[i]])$sigma, segModelsDescriptions[[i]])

      if( i == 1 ) { modelMetricsS2 <- a} else ( modelMetricsS2 <- rbind(modelMetricsS2, a))
  }# for loop
} else ( for ( i in 1:length(vmS2) ){   
      a <- data.frame(extractAIC(mS2[[i]])[1], extractAIC(mS2[[i]])[2], summary(mS2[[i]])$sigma, segModelsDescriptions[[i]])

      if( i == 1 ) { modelMetricsS2 <- a} else ( modelMetricsS2 <- rbind(modelMetricsS2, a))
  }# for loop
)

rownames(modelMetricsS2) <-   paste0( 'm', seq(from =1, to = length(mS2), by = 1), 'S2')  
colnames(modelMetricsS2) <- c('meanRMSE', 'meanBiasMean', 'meanBiasSD', 'AIC.df', 'AIC', 'Sigma', 'Model Description')

modelMetricsS2

#=================================================================================================================
#                                                    Segment 3
#=================================================================================================================
#Validate for a random subset of sites:

randomSitesS3 <- sample(unique(et3$site), size = validateFrac*length(unique(et3$site)))
valData3    <- et3[et3$site %in% randomSitesS3,]

mS3  <- list()
vmS3 <- list()


for (i in 1:length(segModels)){
  print(i/length(segModels))
  
  mS3[[i]] <- lm(as.formula(segModels[[i]]), data=et3)
  
  if (validate){
    vmS3[[i]] <- validateModel( mS3[[i]], et3, valData3 )
    if(createBiasMaps) {makeBiasMap(vmS3[[i]])}
  }
}

#-------------------
#Compare the models:
#-------------------
if(validate){
  for ( i in 1:length(vmS3) ){   
      a <- data.frame(vmS3[[i]]$means, extractAIC(mS3[[i]])[1], extractAIC(mS3[[i]])[2], summary(mS3[[i]])$sigma, segModelsDescriptions[[i]])

      if( i == 1 ) { modelMetricsS3 <- a} else ( modelMetricsS3 <- rbind(modelMetricsS3, a))
  }# for loop

} else ( for ( i in 1:length(vmS3) ){   
      a <- data.frame(extractAIC(mS3[[i]])[1], extractAIC(mS3[[i]])[2], summary(mS3[[i]])$sigma, segModelsDescriptions[[i]])

      if( i == 1 ) { modelMetricsS3 <- a} else ( modelMetricsS3 <- rbind(modelMetricsS3, a))
  }# for loop
)

rownames(modelMetricsS3) <-   paste0( 'm', seq(from =1, to = length(mS3), by = 1), 'S3')  
colnames(modelMetricsS3) <- c('meanRMSE', 'meanBiasMean', 'meanBiasSD', 'AIC.df', 'AIC', 'Sigma', 'Model Description')

modelMetricsS3


#=================================================================================================================
#                                                   Save the model metrics
#=================================================================================================================

# Want to save the metrics but it's a "list" so can't be a .csv for some reason. Savings as .RData for now.
if(validate){
  segmentModelMetrics <- rbind(modelMetricsS2, modelMetricsS3)
  save(segmentModelMetrics, file=paste0(dataOutDir, sourceList,'/segmentModelMetrics.RData' ) )
}
```

```{r Choose models to use and predict stream temp values}
#After model comparison, choose which model to use to predict:

predictionModelNum <- 15

finalModelS2  <- mS2 [[predictionModelNum]]; finalModelS3   <- mS3 [[predictionModelNum]]
finalValModS2 <- vmS2[[predictionModelNum]]; finalValModS3  <- vmS3[[predictionModelNum]]

# Predict stream temperatures:
et2[,c('pred', 'lowr', 'upr')] <- predict (finalModelS2, newdata=et2, interval = "confidence") #default confidence interval (CI) = 95%
et3[,c('pred', 'lowr', 'upr')] <- predict (finalModelS3, newdata=et3, interval = "confidence") #default confidence interval (CI) = 95%

# Predict air/stream temperature slopes:
et2Slopes <- ddply( et2, .(site), summarize, slopeSeg2=coef(lm(pred ~ airTemp))[2])
et3Slopes <- ddply( et3, .(site), summarize, slopeSeg3=coef(lm(pred ~ airTemp))[2])

siteData <- merge(siteData, et2Slopes, by = 'site', all.x = T, sort = F)
siteData <- merge(siteData, et3Slopes, by = 'site', all.x = T, sort = F)

```

```{r graphs of predicted/obs etc}

# In this section we want to add in coloring by state or agency
#   Use:     geom_point(aes(color = factor(site))) +

#Also: "geom_text(aes(label=site))" for looking at specific sites

# Plot all observed vs predicted data:
gPredObs <- 
  ggplot(et3,aes(pred,temp))+
  geom_point(size=0.5) +
    scale_x_continuous(expression(paste("Predicted water temperature (",degree, "C)", sep = "")))+
    scale_y_continuous(expression(paste("Observed water temperature (",degree, "C)", sep = "")))+
  theme_bw(base_size=20) +
  geom_abline(intercept=0,slope=1,color='white')

ggsave( file=paste0(graphsDir, sourceList,'/predObs.png'), plot=gPredObs, dpi=dpiIn , width=8,height=5, units='in' )

summary(lm(temp~pred,et3))

if(validate){
  
  #Plot the mean bias from the validation:
  gMeanBias <- ggplot( finalValModS2$v, aes(biasMean)) +
    geom_histogram()+
    scale_x_continuous('Mean bias for each site/year') +
    scale_y_continuous('Frequency')
  
  #Plot the std dev of the bias from the validation:
  gSDBias <- ggplot( finalValModS2$v, aes(biasMean,biasSD)) +
    geom_point()+
    scale_x_continuous('Mean bias for each site/year') +
    scale_y_continuous('SD bias for each site/year')
  
  gValStats <- arrangeGrob( gMeanBias, gSDBias, ncol=1 )
  
  ggsave( file=paste0(graphsDir, sourceList,'/modelValidation.png'), plot=gValStats, dpi=dpiIn , width=8,height=5, units='in' )

}
```

Models for breakpoints
----------------------
When adding new models:
   1) Follow the structure of previously defined models and name it: "bpmX"
   2) Assign the description of the model and name it: "bpdX"
   3) Add new models to the model lists ("bdModels" and "bpModelsDescriptions")

```{r regression bp~+...}
#This section models the breakpoints as a function of fixed covariates.

#=================================================================================================================
#                                          Define the structure of the breakpoint models
#=================================================================================================================

#Breakpoint Model 1:
#-------------------
bpd1 <- 'No interactions'
bpm1 <- '~LatitudeS + LongitudeS + ForestS + AgricultureLS +  BasinElevationMS + ReachSlopePCNTS + TotDASqKMLS + WetlandOrWaterS + SurficialCoarseCLS +(1|year)'

#Breakpoint Model 2:
#-------------------
bpd2 <- 'Full interactions'
bpm2 <-  '~ ( LatitudeS + LongitudeS + ForestS + AgricultureLS + BasinElevationMS + ReachSlopePCNTS + TotDASqKMLS + WetlandOrWaterS + SurficialCoarseCLS )^2 + (1|year)'

#Breakpoint Model 3: 
#-------------------
bpd3 <- 'Full interactions. Impoundments added.'
bpm3 <-  '~ ( LatitudeS + LongitudeS + ForestS + AgricultureLS + BasinElevationMS + ReachSlopePCNTS + TotDASqKMLS + WetlandOrWaterS + SurficialCoarseCLS + ImpoundmentsOpenSqKMLS )^2 + (1|year)'

# List all of the models for reference later:
bpModels <- list(bpm1, bpm2, bpm3)
bpModelsDescriptions <- list(bpd1, bpd2, bpd3)

#=================================================================================================================
#                                               Breakpoint Models
#=================================================================================================================

bp1mods <- list()
bp2mods <- list()
bp3mods <- list()

for ( i in 1:length(bpModels)){
  
  # Breakpoint Models
  bp1mods[[i]] <- lmer(as.formula(paste('springBP', bpModels[[i]])), data=siteData)
  bp2mods[[i]] <- lmer(as.formula(paste('summerBP', bpModels[[i]])), data=siteData)
  bp3mods[[i]] <- lmer(as.formula(paste('fallBP'  , bpModels[[i]])), data=siteData)
  
  # Breakpoint Model AICs
  spr <- data.frame(extractAIC(bp1mods[[i]])[1], extractAIC(bp1mods[[i]])[2], bpModelsDescriptions[[i]])
  names(spr) <- c('df', 'AIC', 'Model Description')
  if( i == 1 ) { sprBPModelMetrics <- spr} else ( sprBPModelMetrics <- rbind(sprBPModelMetrics, spr))

  smr <- data.frame(extractAIC(bp2mods[[i]])[1], extractAIC(bp2mods[[i]])[2], bpModelsDescriptions[[i]])
  names(smr) <- c('df', 'AIC', 'Model Description')
  if( i == 1 ) { smrBPModelMetrics <- smr} else ( smrBPModelMetrics <- rbind(smrBPModelMetrics, smr))

  fal <- data.frame(extractAIC(bp3mods[[i]])[1], extractAIC(bp3mods[[i]])[2], bpModelsDescriptions[[i]])
  names(fal) <- c('df', 'AIC', 'Model Description')
  if( i == 1 ) { falBPModelMetrics <- fal} else ( falBPModelMetrics <- rbind(falBPModelMetrics, fal)) 
}

sprBPModelMetrics
smrBPModelMetrics
falBPModelMetrics
```

```{r Choose models to use and predict breakpoints}

bpModelNum <- 3

finalModBP1 <- bp1mods[[bpModelNum]]
finalModBP2 <- bp2mods[[bpModelNum]]
finalModBP3 <- bp3mods[[bpModelNum]]

# not sure why need this [allow.new.levels=T] but throws an error otherwise
#may be because year is in the df

#BP1
siteData$bp1Pred <- predict(finalModBP1,newdata=siteData,allow.new.levels=T)
siteData$bp1PredAvgYear <- predict(finalModBP1,newdata=siteData,REform=NA)

#BP2
siteData$bp2Pred <- predict(finalModBP2,newdata=siteData,allow.new.levels=T)
siteData$bp2PredAvgYear <- predict(finalModBP2,newdata=siteData,REform=NA)

#BP3
siteData$bp3Pred <- predict(finalModBP3,newdata=siteData,allow.new.levels=T)
siteData$bp3PredAvgYear <- predict(finalModBP3,newdata=siteData,REform=NA)

#Synchronized Range
siteData$bp1bp3 <- siteData$bp3Pred - siteData$bp1Pred
siteData$bp1bp3AvgYear <- siteData$bp3PredAvgYear - siteData$bp1PredAvgYear

save(siteData,file=paste0(dataOutDir, sourceList,'/siteDataWBPs_', sourceList, '.RData'))
```

```{r Predicted BP graphs}

if ( makePlots ) {

  #Predicted vs observed spring BP:
  #--------------------------------
  gObsPredBP1 <- 
  ggplot(siteData[siteData$springBP>25,], aes(bp1Pred,springBP))+
    geom_point(aes(color = agency))+
    geom_abline(intercept=0,slope=1)+
    scale_x_continuous("Predicted spring breakpoint")+
    scale_y_continuous("Observed spring breakpoint")+
    theme_bw(base_size=20)
  
  ggsave( file=paste0(graphsDir, sourceList,'/gObsPredBP1_.png'), plot=gObsPredBP1, dpi=dpiIn , width=8,height=5, units='in' )
  
  #Predicted vs observed summer BP:
  #--------------------------------
  #need to look into 2008, low observed values
  gObsPredBP2 <- 
  ggplot(siteData[siteData$summerBP>180&siteData$summerBP<240&siteData$year!=2008,], aes(bp2Pred,summerBP))+
    geom_point()+
    geom_abline(intercept=0,slope=1)+
    scale_x_continuous("Predicted summer breakpoint")+
    scale_y_continuous("Observed summer breakpoint")+
    theme_bw(base_size=20) 
  
  ggsave( file=paste0(graphsDir, sourceList,'/gObsPredBP2_.png'), plot=gObsPredBP2, dpi=dpiIn , width=8,height=5, units='in' )
  
  #Predicted vs observed fall BP:
  #------------------------------
  gObsPredBP3 <- 
  ggplot(siteData, aes(bp3Pred,fallBP))+
    geom_point()+
    geom_abline(intercept=0,slope=1)+
    scale_x_continuous("Predicted fall breakpoint")+
    scale_y_continuous("Observed fall breakpoint")+
    theme_bw(base_size=20)#+facet_wrap(~year) 
  
  ggsave( file=paste0(graphsDir, sourceList,'/gObsPredBP3_.png'), plot=gObsPredBP3, dpi=dpiIn , width=8,height=5, units='in' )

}
```

Predict values for selected catchements
1) Predict breakpoints for UpstreamStats
2) Merge bps into Daymet files
3) Identify segements in Daymet files
3) Predict water temp for segments 2,3

```{r Define prediction region, year, covariates used, and catchments.}

#Pick the area you want to predict for. This is done by selection of daymet tiles:
# See Map Here: http://daymet.ornl.gov/sites/default/files/images/Tiles_on_LCC_projection_300dpi_labels.jpg

DaymetTiles <- c(11754, 11755, 11934, 11935, 12114, 12115)

Year <- 2010

#Read in NHD catchments you want to predict for. This should fall within the boundaries of the daymet tiles above.
proj4.NHD  <- "+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs"

catchments <- readShapePoly ( "C:/KPONEIL/USGS/NHDPlusV2/Modified Versions/CTRiverStates_NHDCatchment.shp", proj4string=CRS(proj4.NHD))

features <- catchments$FEATUREID

#Load the observed covariate data:
load(paste0(dataInDir, 'NENY_CovariateData_2014-04-28.RData'))

#Scale the covariates for prediction to match the model inputs:
#--------------------------------------------------------------
#Normal scaling:
UpstreamStats$LatitudeS        <- (UpstreamStats$Latitude        - mean(UpstreamStats$Latitude        ,na.rm=T)) /sd(UpstreamStats$Latitude        ,na.rm=T)
UpstreamStats$LongitudeS       <- (UpstreamStats$Longitude       - mean(UpstreamStats$Longitude       ,na.rm=T)) /sd(UpstreamStats$Longitude       ,na.rm=T)
UpstreamStats$ForestS          <- (UpstreamStats$Forest          - mean(UpstreamStats$Forest          ,na.rm=T)) /sd(UpstreamStats$Forest          ,na.rm=T)
UpstreamStats$BasinElevationMS <- (UpstreamStats$BasinElevationM - mean(UpstreamStats$BasinElevationM ,na.rm=T)) /sd(UpstreamStats$BasinElevationM ,na.rm=T)
UpstreamStats$ReachSlopePCNTS  <- (UpstreamStats$ReachSlopePCNT  - mean(UpstreamStats$ReachSlopePCNT  ,na.rm=T)) /sd(UpstreamStats$ReachSlopePCNT  ,na.rm=T)
UpstreamStats$WetlandOrWaterS  <- (UpstreamStats$WetlandOrWater  - mean(UpstreamStats$WetlandOrWater  ,na.rm=T)) /sd(UpstreamStats$WetlandOrWater  ,na.rm=T)

#Log scaling:
UpstreamStats$AgricultureLS          <- (log(UpstreamStats$Agriculture          + 0.001) - mean(log(UpstreamStats$Agriculture         + 0.001) ,na.rm=T)) / sd(log(UpstreamStats$Agriculture          + 0.001), na.rm=T)
UpstreamStats$TotDASqKMLS            <- (log(UpstreamStats$TotDASqKM            + 0.001) - mean(log(UpstreamStats$TotDASqKM            +0.001) ,na.rm=T)) / sd(log(UpstreamStats$TotDASqKM            + 0.001), na.rm=T)
UpstreamStats$SurficialCoarseCLS     <- (log(UpstreamStats$SurficialCoarseC     + 1    ) - mean(log(UpstreamStats$SurficialCoarseC     + 1   ) ,na.rm=T)) / sd(log(UpstreamStats$SurficialCoarseC     + 1    ), na.rm=T)
UpstreamStats$ImpoundmentsOpenSqKMLS <- (log(UpstreamStats$ImpoundmentsOpenSqKM + 1    ) - mean(log(UpstreamStats$ImpoundmentsOpenSqKM +1    ) ,na.rm=T)) / sd(log(UpstreamStats$ImpoundmentsOpenSqKM + 1    ), na.rm=T)

```

```{r predicted values for select catchments}

#Here "UpstreamStatsCT" has become "predictionStats" and "CTday2010" has become "FullRecord".

#Select the prediction covariates to index:
predictionCovs <- c('FEATUREID', 'LatitudeS','LongitudeS','ForestS', 'AgricultureLS','BasinElevationMS','ReachSlopePCNTS', 'TotDASqKMLS', 'WetlandOrWaterS','SurficialCoarseCLS', 'ImpoundmentsOpenSqKMLS')

predictionStats <- UpstreamStats[ ,names(UpstreamStats) %in% c(predictionCovs, "StreamOrder")]

rm(UpstreamStats, LocalStats)

predictionStats$year <- Year # for bp predictions

# predict bps
#REform=NA uses no REs. Default is to use all REs (year in our case)
predictionStats$bp1 <- predict(finalModBP1,newdata=predictionStats, allow.new.levels = T)
predictionStats$bp2 <- predict(finalModBP2,newdata=predictionStats, allow.new.levels = T)
predictionStats$bp3 <- predict(finalModBP3,newdata=predictionStats, allow.new.levels = T)

predictionStats$bp1bp3 <- predictionStats$bp3 - predictionStats$bp1
##########################

for ( i in 1:length(DaymetTiles)){
  
  print(i)
  
  # Read in daily data for tile:
  #-----------------------------
  setwd("C:/KPONEIL/GitHub/projects/temperatureProject/dataIn/DaymetClimateData")
  
  load(paste0('NHD_DaymetTile_' , DaymetTiles[i], '_', Year, '.RData'))
  
  predictionStatsTile <- predictionStats[predictionStats$FEATUREID %in% FullRecord$FEATUREID, ]

  if(exists('features')){predictionStatsTile <- predictionStatsTile[predictionStatsTile$FEATUREID %in% features, ]}
  if(exists('features')){FullRecord <- FullRecord[FullRecord$FEATUREID %in% features,]}
  
  FullRecord$site <- FullRecord$FEATUREID

  # Merge in bps and assign segments. Do here to make FullRecord smaller:
  #----------------------------------------------------------------------
  FullRecord <- merge( x=FullRecord, y=predictionStatsTile, all.x=T, by = 'FEATUREID' )
  
  #limit FullRecord to between bp1 and bp3 and assign segments
  FullRecord <- FullRecord[FullRecord$dOY > FullRecord$bp1 & 
                         FullRecord$dOY < FullRecord$bp3,]
  FullRecord$segment <- ifelse( FullRecord$dOY > FullRecord$bp1 & 
                               FullRecord$dOY <= FullRecord$bp2, 2,
                        ifelse( FullRecord$dOY > FullRecord$bp2 &
                               FullRecord$dOY <= FullRecord$bp3, 3, NA))
  
  # get rid of site/segment combos with all low # of obs
  counts <- ddply( FullRecord, .(site,segment), summarize, count=length(na.omit(airTemp)))
  FullRecord <- merge( x=FullRecord, y=counts, all.x=T )
  FullRecord <- FullRecord[ FullRecord$count > 3, ]


  # Lag select daymet variables (slow for big datasets)
  #----------------------------
  FullRecord <- FullRecord[order(FullRecord$FEATUREID,FullRecord$dOY),] # just to make sure FullRecord is ordered for the slide function
  
  # airTemp
  FullRecord <- slide(FullRecord, Var = "airTemp", GroupVar = "site", slideBy = -1, NewVar='airTempLagged1')
  FullRecord <- slide(FullRecord, Var = "airTemp", GroupVar = "site", slideBy = -2, NewVar='airTempLagged2')

  # prcp
  FullRecord <- slide(FullRecord, Var = "prcp",    GroupVar = "site", slideBy = -1, NewVar='prcp1')
  FullRecord <- slide(FullRecord, Var = "prcp",    GroupVar = "site", slideBy = -2, NewVar='prcp2')
  FullRecord <- slide(FullRecord, Var = "prcp",    GroupVar = "site", slideBy = -3, NewVar='prcp3')
  
  # Calculate 5-day mean of prcp
  #-----------------------------
  FullRecord$prcp5Day <- NA

  loopSites <- unique(FullRecord$site)
  
  window <- 5
  for (i in 1:length(loopSites)){
  
    currSite <- which(FullRecord$site == loopSites[i])

    FullRecord$prcp5Day[currSite] <- rollapply(FullRecord$prcp[currSite], width=window, fill=NA, mean, align = 'left')
  }

  
  # Log scale the daymet variables:
  #--------------------------------
  FullRecord$sweL      <- log(FullRecord$swe      + 0.001)
  FullRecord$prcpL     <- log(FullRecord$prcp     + 0.001)
  FullRecord$prcpL1    <- log(FullRecord$prcp1    + 0.001)
  FullRecord$prcpL2    <- log(FullRecord$prcp2    + 0.001)
  FullRecord$prcpL3    <- log(FullRecord$prcp3    + 0.001)
  FullRecord$prcp5DayL <- log(FullRecord$prcp5Day + 0.001)

  
  
  
  
  
  # Split the record into segments:
  #--------------------------------
  FR2 <- FullRecord[FullRecord$segment %in% 2,]
  FR3 <- FullRecord[FullRecord$segment %in% 3,]
  
  
  FR2day <- data.frame( FR2[,c('site', 'year.x', 'dOY')], 
                        sweLS = (FR2$sweL-mean(FR2$sweL,na.rm=T))/sd(FR2$sweL,na.rm=T), 
                        daylS = (FR2$dayl-mean(FR2$dayl,na.rm=T))/sd(FR2$dayl,na.rm=T), 
                        sradS = (FR2$srad-mean(FR2$srad,na.rm=T))/sd(FR2$srad,na.rm=T),
                        prcpLS = (FR2$sweL-mean(FR2$sweL,na.rm=T))/sd(FR2$sweL,na.rm=T))
  
  FR3day <- data.frame( FR3[,c('site', 'year.x', 'dOY')], 
                        sweLS = (FR3$sweL-mean(FR3$sweL,na.rm=T))/sd(FR3$sweL,na.rm=T),              
                        daylS = (FR3$dayl-mean(FR3$dayl,na.rm=T))/sd(FR3$dayl,na.rm=T), 
                        sradS = (FR3$srad-mean(FR3$srad,na.rm=T))/sd(FR3$srad,na.rm=T))
  
  FRday <- rbind(FR2day, FR3day)
  
  FullRecord <- merge(FullRecord, FRday, by = c('site', 'year.x', 'dOY'), all.x = T, sort = F)
  
  
  
  #############################
  # predict daily water temp
  
  FullRecord$predTemp <- ifelse( FullRecord$segment == 2,
                                 predict(finalModelS2,FullRecord),
                         ifelse( FullRecord$segment == 3,
                                 predict(finalModelS3,FullRecord), NA ) )

  #############################
  # get CI for daily temp prediction
  seg2mod <- data.frame(as.matrix(FullRecord[,c('site','segment', 'dOY', 'bp2'), ]), predict(finalModelS2,FullRecord, interval = "confidence"))
  seg2 <- seg2mod[seg2mod$segment == 2,]
  
  seg3mod <- data.frame(as.matrix(FullRecord[,c('site','segment', 'dOY', 'bp2'), ]), predict(finalModelS3,FullRecord, interval = "confidence"))
  seg3 <- seg3mod[seg3mod$segment == 3,]

  segNAmod <- data.frame(as.matrix(FullRecord[,c('site','segment', 'dOY', 'bp2'), ]), matrix(NA, nrow = nrow(FullRecord), ncol = 3))
  segNA <- segNAmod[!(segNAmod$segment %in% c(2,3)),]
  colnames(segNA) <- c('site','segment', 'dOY', 'fit', 'lwr', 'upr')

  CIPreds <- rbind(seg2, seg3, segNA)
  CIPreds <- CIPreds[order(CIPreds$site,CIPreds$dOY),] 
  
  # Estimate slopes of air/water for each site:
  #--------------------------------------------
  # Can't do this directly in the big regressions because other daily covariates besides airTemp are included (swe, dayl, srad)

  slopes <- ddply( FullRecord, .(site,segment), summarize, slope=coef(lm(predTemp ~ airTemp))[2])    
  
  slopesMelt <- melt(slopes,id.vars=c('site','segment'))
  slopesCast <- cast(slopesMelt,site~segment)
  names(slopesCast) <- c('site','slopeSeg2','slopeSeg3')
  FullRecord <- merge( x=FullRecord, y=slopes, all.x=T, by = c('site','segment') )
  
  predictionStatsTile$site <- predictionStatsTile$FEATUREID
  predictionStatsTile <- merge( x=predictionStatsTile, y=slopesCast, all.x=T, by = c('site') )
  #ggplot(slopes, aes(slope)) + geom_histogram()+facet_wrap(~segment)
  
  #Get the confidence interval for the slopes
  slopesCI <- ddply( FullRecord, .(site,segment), summarize, range=confint(lm(predTemp ~ airTemp), 'airTemp', level = 0.95))
  
  slopesCI$slopeCI <- slopesCI$range[,2] - slopesCI$range[,1]

  slopesCI <- slopesCI[,c('site', 'segment', 'slopeCI')]
  
  slopesMeltCI <- melt(slopesCI,id.vars=c('site','segment'))
  slopesCastCI <- cast(slopesMeltCI,site~segment)
  names(slopesCastCI) <- c('site','slopeSeg2CI','slopeSeg3CI')
  
  predictionStatsTile <- merge( x=predictionStatsTile, y=slopesCastCI, all.x=T, by = c('site') )


  #Calculate the temperature prediction at the Summer BP:
  #------------------------------------------------------
  summerMax <- ddply( FullRecord, .(site), summarize, predTempAtBP2 = predTemp[which(dOY == round(bp2))] )
  predictionStatsTile <- merge ( predictionStatsTile, summerMax, by = 'site', all.x = T, sort = F)
  
  ##Calculate the CI of the temperature prediction at the Summer BP:
  ##----------------------------------------------------------------  
  summerMaxCI <- ddply( CIPreds, .(site), summarize, predTempAtBP2CI = upr[which(dOY == round(bp2))] - lwr[which(dOY == round(bp2))] )
  predictionStatsTile <- merge ( predictionStatsTile, summerMaxCI, by = 'site', all.x = T, sort = F)

  
  TempPreds <- predictionStatsTile[,c('FEATUREID', 'bp1', 'bp2', 'bp3', 'bp1bp3', 'slopeSeg2', 'slopeSeg2CI',  'slopeSeg3', 'slopeSeg3CI', 'predTempAtBP2', 'predTempAtBP2CI')]
    
  if ( i == 1 ) {Predictions <- TempPreds}  else (Predictions <- rbind(Predictions, TempPreds))

  save(FullRecord, file=paste0(dataOutDir, sourceList, '/DaymetClimateDataNHD_DaymetTile' , DaymetTiles[i], '_', Year, 'AfterR_', sourceList, '.RData'))
}

```


```{r Write out prediction files for ArcGIS}
Predictions <- merge(Predictions, predictionStats[,c('FEATUREID', 'StreamOrder')], by = 'FEATUREID', all.x = T, sort = F)

#Write out prediction files for ArcGIS:
#--------------------------------------
names(Predictions) <- c('FEATUREID', 'SpringBP', 'SummerBP', 'FallBP', 'SyncTemps', 'RiseSlope', 'RiseSloCI', 'FallSlope', 'FallSloCI', 'SummerMaxT', 'SumrMaxTCI', 'StreamOrder')

Predictions <- replace(Predictions, is.na(Predictions), -9999)

write.csv(Predictions, file = paste0(dataOutDir, sourceList, '/BP_FullPredictions', Year, '.csv'), row.names = F )
write.dbf(Predictions, file = paste0(dataOutDir, sourceList, '/BP_FullPredictions', Year, '.dbf'))

```

```{r Write out predictions for headwaters only (stream order 3 or less)}
streamOrderTrim <- Predictions

streamOrderTrim[ streamOrderTrim$StreamOrder > 3 , c('SpringBP', 'SummerBP', 'FallBP', 'SyncTemps', 'RiseSlope', 'RiseSloCI', 'FallSlope', 'FallSloCI', 'SummerMaxT', 'SumrMaxTCI')] <- -9999

write.csv(streamOrderTrim, file = paste0(dataOutDir, sourceList, '/BP_HeadwatersPredictions', Year, '.csv'), row.names = F )
write.dbf(streamOrderTrim, file = paste0(dataOutDir, sourceList, '/BP_HeadwatersPredictions', Year, '.dbf'))
```



#---------------------------------------------------------------------

#                            OTHER MODELS

#---------------------------------------------------------------------




#Model 3: Selected interactions
#---------------------------------------------------------------------
m3 <- 'temp~airTemp + airTempLagged1 + airTempLagged2 + LatitudeS + LongitudeS + BasinElevationMS + ReachSlopePCNTS + 
             TotDASqKMLS + SurficialCoarseCLS + ForestS + AgricultureLS + WetlandOrWaterS + ImpoundmentsOpenSqKMLS +
             daylS + sradS + sweLS + 
             airTemp*ReachSlopePCNTS + airTemp*TotDASqKMLS + airTemp*WetlandOrWaterS + 
             airTemp*ImpoundmentsOpenSqKMLS + airTemp*sweLS + 
             airTempLagged1 + airTempLagged1*ReachSlopePCNTS + airTempLagged1*TotDASqKMLS + airTempLagged1*WetlandOrWaterS + 
             airTempLagged1*ImpoundmentsOpenSqKMLS + airTempLagged1*sweLS + 
             airTempLagged2 + airTempLagged2*ReachSlopePCNTS + airTempLagged2*TotDASqKMLS + airTempLagged2*WetlandOrWaterS + 
             airTempLagged2*ImpoundmentsOpenSqKMLS + airTempLagged2*sweLS + 
             ReachSlopePCNTS*ImpoundmentsOpenSqKMLS + ReachSlopePCNTS*daylS + ReachSlopePCNTS*sradS +
             TotDASqKMLS*ImpoundmentsOpenSqKMLS + TotDASqKMLS*daylS + TotDASqKMLS*sradS +
             SurficialCoarseCLS*ForestS +   
             ForestS*daylS + ForestS*sradS + ForestS*sweLS +
             WetlandOrWaterS*daylS + WetlandOrWaterS*sradS + 
             ImpoundmentsOpenSqKMLS*daylS + ImpoundmentsOpenSqKMLS*sradS + 
             daylS*sweLS + 
             sradS*sweLS'

#Model 4: Two-way interactions  without Daymet data.
#---------------------------------------------------------------------
m4 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              #segment+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS 
              )^2'



#Model 7: Selected interactions with prcp
#---------------------------------------------------------------------
m7 <- 'temp~airTemp + airTempLagged1 + airTempLagged2 + LatitudeS + LongitudeS + BasinElevationMS + ReachSlopePCNTS + 
             TotDASqKMLS + SurficialCoarseCLS + ForestS + AgricultureLS + WetlandOrWaterS + ImpoundmentsOpenSqKMLS +
             daylS + sradS + sweLS + prcpLS +
             airTemp*ReachSlopePCNTS + airTemp*TotDASqKMLS + airTemp*WetlandOrWaterS + 
             airTemp*ImpoundmentsOpenSqKMLS + airTemp*sweLS + 
             airTempLagged1 + airTempLagged1*ReachSlopePCNTS + airTempLagged1*TotDASqKMLS + airTempLagged1*WetlandOrWaterS + 
             airTempLagged1*ImpoundmentsOpenSqKMLS + airTempLagged1*sweLS + 
             airTempLagged2 + airTempLagged2*ReachSlopePCNTS + airTempLagged2*TotDASqKMLS + airTempLagged2*WetlandOrWaterS + 
             airTempLagged2*ImpoundmentsOpenSqKMLS + airTempLagged2*sweLS + 
             ReachSlopePCNTS*ImpoundmentsOpenSqKMLS + ReachSlopePCNTS*daylS + ReachSlopePCNTS*sradS +
             TotDASqKMLS*ImpoundmentsOpenSqKMLS + TotDASqKMLS*daylS + TotDASqKMLS*sradS +
             SurficialCoarseCLS*ForestS +   
             ForestS*daylS + ForestS*sradS + ForestS*sweLS +
             WetlandOrWaterS*daylS + WetlandOrWaterS*sradS + 
             ImpoundmentsOpenSqKMLS*daylS + ImpoundmentsOpenSqKMLS*sradS + 
             daylS*sweLS + 
             sradS*sweLS +
             prcpLS*BasinElevationMS'




