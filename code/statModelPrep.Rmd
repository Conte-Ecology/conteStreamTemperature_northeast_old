Overall approach: 
1) Define bps and slopes by segment for each site/year combo in temperatureSegmentedBreakPointAnalysis.Rmd. 
2) Model slopes for each segment (2=sp-sum, 3=sum-autumn) as a function of airTemp and fixed covariates. This gets predicted water temp as a function of airTemp and covariates, but does not identify bps.
3) Model bps as a fucntion of covariates including swe for bp1.
4) Predict water temp as function of airTemp and covariates between bps for each prediction site
5) Summarize data for slopes btw bps

Note: run temperatureSegmentedBreakPointAnalysis.Rmd before running this script

```{r load libs}
rm(list=ls())

library(ggplot2)
library(relaimpo)
library(lme4)
library(DataCombine) # for the slide function
library(plyr)
library(reshape)
library(ggmap)
library(foreign)
library(maptools)
library(gridExtra)
library(nlme)
library(zoo)

setwd('/Users/Dan/Documents/Research/Stream_Climate_Change/temperatureProject/')
#setwd('C:/KPONEIL/GitHub/projects/temperatureProject/')


#baseDir <- 'C:/KPONEIL/GitHub/projects/temperatureProject/'
baseDir <- '/Users/Dan/Documents/Research/Stream_Climate_Change/temperatureProject/'

dataInDir <- paste0(baseDir, 'dataIn/')
dataOutDir <- paste0(baseDir, 'dataOut/')
graphsDir <- paste0(baseDir, 'graphs/')

source(paste0(baseDir, 'code/functions/temperatureModelingFunctions.R'))

```

Which agencies do you want to pull data from?
```{r Define data sources and other options}

#If removeSelectSites = TRUE, then the file with the list of sites needs to be specified.
removeSelectSites <- F
sitesToRemove <- paste0(baseDir, 'dataIn/sitesToRemoveAllNE.csv')

#Do you want all of the plots made?
makePlots <- F

#Use validation?
validate = T
  
#If validating:
  # Choose fraction of total # of sites:
  validateFrac <- 0.1

  #Do you want to create bias maps? (Internet connection required)
  createBiasMaps <- F

#Data source agencies?
CTDEP  <- T
MAFW   <- T
MAUSGS <- T
NHFG   <- T
NHDES  <- T
USFS   <- T
VTFWS  <- T
MEDMR  <- T
MTUSGSYellowstone <- F
MTUSGSGlacier <- F

#global vars
dpiIn <- 400

```

```{r load data} 

#Set up data list.
sourceChoice <- list( CTDEP,   MAFW,   MAUSGS,   NHFG,   NHDES,   MEDMR,   USFS,   VTFWS,    MTUSGSYellowstone,   MTUSGSGlacier )
sourceNames  <- c   ('CTDEP', 'MAFW', 'MAUSGS', 'NHFG', 'NHDES', 'MEDMR', 'USFS', 'VTFWS',  'MTUSGSYellowstone', 'MTUSGSGlacier')

dataSource <- sourceNames[sourceChoice == T]

#sourceList <- paste0(paste0(dataSource, collapse = '_'))
sourceList <- paste0(paste0(dataSource, collapse = '_'), '_2014-06-04')

#Load "et" for the agencies:
load(paste0(dataOutDir, sourceList,  '/et_', sourceList, '.RData'))

#Pull duplicate columns so they doesn't get doubled up
et <- et[,-which(names(et) %in% c('Latitude', 'Longitude', 'StreamOrder', 'HUC_4', 'HUC_8.x', 'HUC_8.y', 'HUC_12', 'agency'))]

#Load in covariate data to merge into slopes df [no day data]
for ( i in 1:length(dataSource)){

  #Load covariate data to be merged into slopes df [no day data]
  load(paste0(dataInDir, dataSource[i], '/covariateData_', dataSource[i], '.RData')) #Fixed over time
  covariateData$agency <- paste(dataSource[i])
  dim(covariateData)
  if ( i == 1) {covs <- covariateData} else (covs <- rbind(covs, covariateData))
  
  #Load daymet climate data to be merged into et:
  load(paste0(dataInDir, dataSource[i], '/streamTempSitesObservedClimateData_', dataSource[i], '.RData')) 
  
  #Pull out the columns needed:
  masterData <- masterData[, c('site', 'year', 'dOY', 'date', 'dayl', 'srad', 'swe', 'tmax', 'tmin', 'vp', 'prcp')]
  if ( i == 1) {newDay <- masterData} else ( newDay <- rbind(newDay, masterData) )
}

masterData    <- newDay
covariateData <- covs

#Merge climate data into main dataframe:
et <- merge(et, masterData, by = c('site', 'date', 'year', 'dOY'), all.x=T, sort = F )

et$flow <- NA
et$tAirMin <- et$tmin; et$tAirMax <- et$tmax

#Overwrite NaNs with NAs:
covariateData <- replace(covariateData, is.na(covariateData), NA)

#Make site a character string so the "merge" function works:
covariateData$site <- as.character(covariateData$site)

##For testing
#testSites <- c("MAUSGS_WB_JIMMY", "MAUSGS_SEC_45_DL", "MAUSGS_WEST_BROOK", "MAUSGS_SEC_6_DL", "MAUSGS_SEC_30_DL", "MAUSGS_WB_OBEAR", "MAUSGS_WB_MITCHELL")

#et <- et[et$site %in% testSites,]
#covariateData <- covariateData[covariateData$site %in% testSites,]

#=================================================================================================================
#Scale the variables used in the model. Some get log-scaled depending on their distribution over the sites:
#=================================================================================================================
# Standard scaling:
covariateData$LatitudeS              <- (covariateData$Latitude        - mean(covariateData$Latitude       , na.rm=T)) / sd(covariateData$Latitude       , na.rm=T)
covariateData$LongitudeS             <- (covariateData$Longitude       - mean(covariateData$Longitude      , na.rm=T)) / sd(covariateData$Longitude      , na.rm=T)
covariateData$ForestS                <- (covariateData$Forest          - mean(covariateData$Forest         , na.rm=T)) / sd(covariateData$Forest         , na.rm=T)
covariateData$BasinElevationMS       <- (covariateData$BasinElevationM - mean(covariateData$BasinElevationM, na.rm=T)) / sd(covariateData$BasinElevationM, na.rm=T)
covariateData$ReachSlopePCNTS        <- (covariateData$ReachSlopePCNT  - mean(covariateData$ReachSlopePCNT , na.rm=T)) / sd(covariateData$ReachSlopePCNT , na.rm=T)
covariateData$WetlandOrWaterS        <- (covariateData$WetlandOrWater  - mean(covariateData$WetlandOrWater , na.rm=T)) / sd(covariateData$WetlandOrWater , na.rm=T)

# Log scaling:
covariateData$AgricultureLS          <- (log(covariateData$Agriculture          + 0.001) - mean(log(covariateData$Agriculture          + 0.001), na.rm=T)) / sd(log(covariateData$Agriculture          + 0.001), na.rm=T)
covariateData$TotDASqKMLS            <- (log(covariateData$TotDASqKM            + 0.001) - mean(log(covariateData$TotDASqKM            + 0.001), na.rm=T)) / sd(log(covariateData$TotDASqKM            + 0.001), na.rm=T)
covariateData$SurficialCoarseCLS     <- (log(covariateData$SurficialCoarseC     + 1    ) - mean(log(covariateData$SurficialCoarseC     + 1    ), na.rm=T)) / sd(log(covariateData$SurficialCoarseC     + 1    ), na.rm=T)
covariateData$ImpoundmentsOpenSqKMLS <- (log(covariateData$ImpoundmentsOpenSqKM + 1    ) - mean(log(covariateData$ImpoundmentsOpenSqKM + 1    ), na.rm=T)) / sd(log(covariateData$ImpoundmentsOpenSqKM + 1    ), na.rm=T)

et <- merge(et, covariateData, by = 'site', all.x=T, sort = F )

#====================================================================================================
#Remove slected site/years that have errors in breakpoint assignment (chosen via visual examination):
#====================================================================================================
if( removeSelectSites ) {
  
  removeSites <- read.csv(sitesToRemove)
  
  removeSites$site <- as.character(removeSites$site)
  
  et <-  et[!(et$site %in% removeSites$site & et$year %in% removeSites$year),]
}
#====================================================================================================

#Get BPs out of et
bp <- unique(et[,c('site','year','springBP','summerBP','fallBP')]  ) #, 'Latitude', 'Longitude'
bp <- bp[is.finite(bp$springBP) | is.finite(bp$summerBP) | is.finite(bp$fallBP),]
bp$site <- as.character(bp$site) #for merging
  
siteData <- merge( x = bp, y = covariateData, by = 'site', all.x=T )

# turn Inf to NA in bps
siteData[!is.finite(siteData$springBP),'springBP'] <- NA
siteData[!is.finite(siteData$summerBP),'summerBP'] <- NA
siteData[!is.finite(siteData$fallBP),'fallBP'] <- NA

# merge in count of days
obsBySiteYear <- ddply(et, .(site,year), summarize,count=length(!is.na(temp)))
siteData <- merge(x=siteData, y=obsBySiteYear, all.x=T)

```

```{r lag airTemp & prcp}
et <- et[order(et$count),] # just to make sure et is ordered for the slide function

# airTemp
et <- slide(et, Var = "airTemp", GroupVar = "site", slideBy = -1, NewVar='airTempLagged1')
et <- slide(et, Var = "airTemp", GroupVar = "site", slideBy = -2, NewVar='airTempLagged2')

# prcp
et <- slide(et, Var = "prcp", GroupVar = "site", slideBy = -1, NewVar='prcpLagged1')
et <- slide(et, Var = "prcp", GroupVar = "site", slideBy = -2, NewVar='prcpLagged2')
et <- slide(et, Var = "prcp", GroupVar = "site", slideBy = -3, NewVar='prcpLagged3')

```

```{r save et for use in analysis}

save(et, file=paste0(dataOutDir, 'et.RData'))

```

Left out to save time:
----------------------------------------------------------------------------------------------------------------------------------------------------------------
# 5-day mean of prcp 
siteYearCombos <- unique(et[,c('site','year')])

et$prcp5Day <- NA

window <- 5
for (i in 1:nrow(siteYearCombos)){

  print(c(i,as.character(siteYearCombos$site[i]),siteYearCombos$year[i],i/nrow(siteYearCombos)))
  
  currSite <- which(et$site == as.character(siteYearCombos$site[i]) & et$year == siteYearCombos$year[i] )

  #Need this so sites with very short records don't crash the loop.
  if(length(currSite) >= window){currMean <-  rollapply(et$prcp[currSite], width=window, fill=NA, mean, align = 'left')} else(currMean <- NA)
  
  et$prcp5Day[currSite] <- currMean
}
----------------------------------------------------------------------------------------------------------------------------------------------------------------

```{r check out data}
#pairs(~Latitude+Longitude+Forest+ Impervious+ Agriculture+ BasinElevationM+ ReachSlopePCNT+ TotDASqKM+ WetlandOrWater+ SurficialCoarseC,data=et)
#Latitude, Longitude, Forest, Impervious, Agriculture, BasinElevationM, ReachSlopePCT, TotDASqKM, WetlandOrWater, SurficialCoarseC

if(makePlots) {

  #Makes barcode looking plot of data records:
  #-------------------------------------------
  gTile <- 
  ggplot(siteData,aes(site,year,z=any(c(!is.na(springBP),!is.na(summerBP),!is.na(fallBP)))))+
    scale_x_discrete('Site')+
    scale_y_continuous('Year')+
    theme_bw(base_size=20) + 
      theme(axis.text.x = element_blank())+
    geom_tile()
  
  ggsave( file=paste0(graphsDir, sourceList, '/gTile.png'), plot=gTile, dpi=dpiIn , width=8,height=5, units='in' )
  
  #Colors by number of observations?
  #---------------------------------
  gTileHeat <- 
  ggplot(siteData,aes(site,year,z=count))+
    geom_tile(aes(fill=count))+
    scale_x_discrete('Site')+
    scale_y_continuous('Year')+ 
    theme(axis.text.x = element_blank())
  
  ggsave( file=paste0(graphsDir, sourceList,'/gTileHeat.png'), plot=gTileHeat, dpi=dpiIn , width=8,height=5, units='in' )

}

```

```{r scale daymet vars and split et into rising and falling segments}

#List the variables you want logged.
logVariables <- c('swe', 'prcp', 'prcpLagged1', 'prcpLagged2', 'prcpLagged3')# 'prcp5Day'

#List the variables you want scaled. (If you want the logged ones scaled include them as well)
scaleVariables <- c('swe', 'dayl', 'srad', 'prcp', 'prcpLagged1', 'prcpLagged2', 'prcpLagged3')# 'prcp5Day'

# Run the scaling funtion:
logScaledData <- loggAndScaleDaymet(et, logVariables, scaleVariables)

# Rename output:
et  <- logScaledData[[1]] # Main dataframe
et2 <- logScaledData[[2]] # Segment 2
et3 <- logScaledData[[3]] # Segment 3
```

```{r rescale deymet variables without separating into different segments}
et$sweL <- log(et$swe + 0.001)

# Scale daymet vaiables in segment 2:
et$daylS <- (et$dayl - mean(et$dayl,na.rm=T)) / sd(et$dayl, na.rm=T)
et$sradS <- (et$srad - mean(et$srad,na.rm=T)) / sd(et$srad, na.rm=T)
et$sweLS <- (et$sweL - mean(et$sweL,na.rm=T)) / sd(et$sweL, na.rm=T)

 #Make sure all values aren't zero if you want to use this in stats models:


```


```{r regression with nlme sin-cos}
library(nlme)

system.time(lmeTest <- lme(temp ~ airTemp+
                 LatitudeS+LongitudeS+
                 ForestS, random = ~ 1|site, data = et, method = "ML", na.action = "na.omit")) # |year/site ??? 186s (3 min)
summary(lmeTest)

et$jDay <- as.numeric(as.Date(et$date.x, format= "%m/%d/%Y")) + 2440588

# system.time(lmeTest2 <- lme(temp ~ airTemp+
                 LatitudeS+LongitudeS+
                 ForestS, random = ~ 1|site, data = et, method = "ML", na.action = "na.omit", correlation = corCAR1(form = ~jDay|site))) # crashed RStudio
summary(lmeTest2)

system.time(lmeTest3 <- lme(temp~airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS, random = ~ 1|site, data = et, method = "ML", na.action = "na.omit"))
summary(lmeTest3)

system.time(lmeTest4 <- lme(temp~airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS, random = ~ 1 + airTemp+airTempLagged1+airTempLagged2|site, data = et, method = "ML", na.action = "na.omit"))
summary(lmeTest4)

system.time(lmeTest5 <- lme(temp~airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS, random = ~ 1 + airTemp+airTempLagged1+airTempLagged2|site, data = et, method = "ML", na.action = "na.omit", correlation = corCAR1(form = ~date.x|site)))
summary(lmeTest5)


system.time(lmeTest5 <- lme(temp~airTemp+airTempLagged1+airTempLagged2+ daylS + sradS + sweLS, random = ~ 1 + airTemp+airTempLagged1+airTempLagged2|site, data = et, method = "ML", na.action = "na.omit", correlation = corCAR1(form = ~date|site)))
summary(lmeTest5)

rmse(resid(lmeTest5))
mae(resid(lmeTest5))

system.time(lmeTest5 <- lme(temp~airTemp+airTempLagged1+airTempLagged2+ daylS + sradS + sweLS, random = ~ 1 + airTemp+airTempLagged1+airTempLagged2|site, data = et, method = "ML", na.action = "na.omit", correlation = corCAR1(form = ~date|site)))
summary(lmeTest5)

rmse(resid(lmeTest5))
mae(resid(lmeTest5))

# Function that returns Root Mean Squared Error
rmse <- function(error)
{
    sqrt(mean(error^2))
}
 
# Function that returns Mean Absolute Error
mae <- function(error)
{
    mean(abs(error))
}

# Example of invocation of functions
rmse(resid(lmeTest3))
mae(resid(lmeTest3))

### Analysis ###

# "A word of warning is to not get too carried away with accounting for autocorrelation in your data: adding correlation structures with nlme is easy in principle but difficult in practice, and the benefits to model fitting of trying out different correlation structures are usually small, especially with models already containing different spatial or temporal levels.  With a large dataset and complicated model, adding autocorrelation can also add significantly to computation time: for the full temperature models I constructed for the Smokies, ML fitting took over a week on a high-end linux workstation." - Fridley: http://plantecology.syr.edu/fridley/bio793/mixed2.html
# (sin(0.01721421*julian) + cos(0.01721421*julian))

# STEP 1: Try full, beyond optimal model to get a working autocorrelation
system.time(lmefull <- lme(temp ~ airTemp+airTempLagged1+airTempLagged2+
                 LatitudeS+LongitudeS+
                 ForestS+ 
                 BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
                 WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
                 daylS + sradS + sweLS +
                 sin(2*pi/360*dOY) + cos(2*pi/360*dOY), random = ~ sin(2*pi/360*dOY) + cos(2*pi/360*dOY) + airTemp + airTempLagged1 + airTempLagged2 + ForestS |site, data = et, method = "ML", na.action = "na.omit")) # |year/site ???
summary(lmefull)

# First try basic autoregressive decomposition - simple but less likely to work with an overparameterized model - same as ARMA(1,0) (~ minutes)
system.time(lmefullCAR1 <- update(lmefull, correlation = corCAR1(form = ~dOY|site))) # or corCAR1(form = ~Julian|site) to have correlation across years at a site

anova(lmefull, lmefullCAR1) 

# STEP 2: Model the random effects while keeping beyond optimal fixed effects

system.time(lmeR1CAR1 <- update(lmefullCAR1, random = ~1|block/iButton)) #  - random intercept

system.time(lmeR2CAR1 <- update(lmefullCAR1, random = ~tot.sol|block/iButton)) # 

system.time(lmeR3CAR1 <- update(lmefullCAR1, random = ~rad|block/iButton)) # 

system.time(lmeR4CAR1 <- update(lmefullCAR1, random = ~ndvi|block/iButton)) # 

system.time(lmeR5CAR1 <- update(lmefullCAR1, random = ~synmaxT|block/iButton)) # 

AIC(lmefullCAR1, lmeR1CAR1, lmeR2CAR1, lmeR3CAR1, lmeR4CAR1, lmeR5CAR1)   

system.time(lmeR6CAR1 <- update(lmefullCAR1, random = ~synmaxT + tot.sol|block/iButton)) # 

anova(lmeR5CAR1, lmeR6CAR1)
anova(lmeR5CAR1, lmefullCAR1)

system.time(lmeR7CAR1 <- update(lmefullCAR1, random = ~synmaxT + rad|block/iButton)) # 

anova(lmefullCAR1, lmeR7CAR1) # use lmeR7CAR1

system.time(lmeR8CAR1 <- update(lmefullCAR1, random = ~synmaxT + ndvi|block/iButton)) # 

AIC(lmefullCAR1, lmeR1CAR1, lmeR2CAR1, lmeR3CAR1, lmeR4CAR1, lmeR5CAR1, lmeR6CAR1, lmeR7CAR1, lmeR8CAR1)


# Try various ARMA models to account for temporal autocorrelation of residuals
# try p = 2 q = 1
system.time(lmeR7ARMA21 <- update(lmeR7CAR1, correlation = corARMA(p = 2, q = 1, form = ~julian|block/iButton), na.action = na.exclude)) # 

# try p = 1 q = 1
system.time(lmeR7ARMA11 <- update(lmeR7CAR1, correlation = corARMA(p = 1, q = 1, form = ~julian|block/iButton), na.action = na.exclude)) #

anova(lmeR7CAR1, lmeR7ARMA11)
anova(lmefullCAR1, lmeR7ARMA11) # best beyond optimal model = lmeR7ARMA11

# STEP 3: Model selection for level 1 variables
# Level 1 covariates: synmaxT, swv, cwv, rad

```



```{r Dynamic Linear Models}
# Reference: Petris et al. Dynamic Linear Models with R. Book

library(dlm)

```


```{r GAMM}
library(gamm4)


library(mgcv)

et$jDay <- as.numeric(as.Date(et$date.x, format = "%m%d%Y")) + 2440588
et$jDayS <- (et$jDay - mean(et$jDay, na.rm = T)) / (sd(et$jDay, na.rm = T))

system.time(m1Gamm <- gamm(temp ~ s(jDayS), data = et))
summary(m1Gamm$gam, cor = FALSE)
plot(m1Gamm$gam)
summary(m1Gamm$lme)

system.time(m2Gamm <- gamm(temp ~ s(jDay), data = et))
summary(m2Gamm$gam, cor = FALSE)
plot(m2Gamm$gam)
summary(m2Gamm$lme)

m2GammPred <- predict(m2Gamm$gam, se = FALSE, type = "response")
plot(et$date.x, et$temp, cex = 0.1, pch = 16) # some water temps well below 0
I <- order(et$date.x)
lines(et$date.x[I], m2GammPred[I], col = 'red')

system.time(m2Gamm <- gamm(temp ~ s(jDayS), data = et))
summary(m2Gamm$gam, cor = FALSE)
plot(m2Gamm$gam)
summary(m2Gamm$lme)
  
system.time(m3Gamm <- gamm(temp ~  s(year) + s(dOY), random = list(site =~1), data = et)) # 13 min
summary(m3Gamm$gam)
summary(m3Gamm$lme)
plot(m3Gamm$gam)

system.time(m4Gamm <- gamm(temp ~ airTemp+airTempLagged1+airTempLagged2+ LatitudeS+LongitudeS+ ForestS+ BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ daylS + sradS + sweLS + s(year) + s(dOY), random = list(site =~1), data = et))
summary(m4Gamm$gam)
summary(m4Gamm$lme)
plot(m4Gamm$gam)

system.time(m5Gamm <- gamm(temp ~ airTemp+airTempLagged1+airTempLagged2+ LatitudeS+LongitudeS+ ForestS+ BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ daylS + sradS + sweLS + s(year) + s(dOY), random = list(site =~1 + airTemp), data = et)) # 26 min
summary(m5Gamm$gam)
summary(m5Gamm$lme)
plot(m5Gamm$gam)

system.time(m5GammIdent <- gamm(temp ~ airTemp+airTempLagged1+airTempLagged2+ LatitudeS+LongitudeS+ ForestS+ BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ daylS + sradS + sweLS + s(year) + s(dOY), random = list(site =~1 + airTemp), data = et, weights = varIdent(form =~1 | site))) # 10 min
summary(m5GammIdent$gam)
summary(m5GammIdent$lme)
plot(m5GammIdent$gam)

system.time(m5GammPow <- gamm(temp ~ airTemp+airTempLagged1+airTempLagged2+ LatitudeS+LongitudeS+ ForestS+ BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ daylS + sradS + sweLS + s(year) + s(dOY), random = list(site =~1 + airTemp), data = et, weights = varPower(form =~dOY))) # 37 min
summary(m5GammPow$gam)
summary(m5GammPow$lme)
plot(m5GammPow$gam)

system.time(m6Gamm <- gamm(temp ~ airTemp+airTempLagged1+airTempLagged2+ LatitudeS+LongitudeS+ ForestS+ BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ daylS + sradS + sweLS + s(year) + s(dOY), random = list(site =~1 + airTemp + s(dOY)), data = et)) #
summary(m6Gamm$gam)
summary(m6Gamm$lme)
plot(m6Gamm$gam)


# try separate smoothers for HUC 4 or HUC 8 or based on latitudinal groupings

```


```{r hts}
library(hts)
help('hts')
vignette('hts')

bts <- ts(5 + matrix(sort(rnorm(500)), ncol=5, nrow=100))
y <- hts(bts, nodes=list(2, c(3, 2)))
summary(y)

ally <-  aggts(y) # Returns all series in the hierarchy
somey <-  aggts(y, levels = c(0, 2)) # Returns time series at levels 0 and 2
S <- smatrix(y) # Returns the summing matrix

plot(y, levels = c(0, 1, 2))


# Forecast 10-step-ahead using the bottom-up method
infantforecast <- forecast(infantgts, h=10, method="bu")
# plot the forecasts including the last ten historical years
plot(infantforecast, include=10)

allts_infant <- aggts(infantgts)
allf <- matrix(, nrow=10, ncol=ncol(allts_infant))
# Users can select their preferred time-series forecasting method
# for each time series
for(i in 1:ncol(allts_infant))
  allf[,i] <- forecast(auto.arima(allts_infant[,i]), h=10, PI=FALSE)$mean
allf <- ts(allf, start=2004)
# combine the forecasts with the group matrix to get a gts object
g <- matrix(c(rep(1, 8), rep(2, 8), rep(1:8, 2)), nrow = 2, byrow = T)
y.f <- combinef(allf, groups = g)


# set up the training sample and testing sample
data <- window(infantgts, start=1933, end=1993)
test <- window(infantgts, start=1994, end=2003)
forecast <- forecast(data, h=10, method="bu")
# calculate ME, RMSE, MAE, MAPE, MPE and MASE
accuracy.gts(forecast, test)

```


```{r FDA}


```




Models for rising & falling segments
------------------------------------
When adding new models:
   1) Follow the structure of previously defined models and name it: "mX"
   2) Assign the description of the model and name it: "dX"
   3) Add new models to the model lists ("segModels" and "segModelsDescriptions")
  
```{r regression temp~airTemp+...}
#This section explores modeling temperature using fixed covariate data and time series of climate data.
# Models are created by segment (warming and cooling).

#=================================================================================================================
#                                         Define the Segment Models:
#=================================================================================================================

#Model 1:
d1 <- 'Air temp & lags only'
#---------------------------
m1 <- 'temp~airTemp+airTempLagged1+airTempLagged2'


#Model 2:

d2 <- 'All main effects'
#-----------------------
m2 <-'temp~airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS'


#Model 3:
d3 <- 'All two-way interactions'
#-------------------------------
m3 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS)^2'

#Model 4:
d4 <- 'All main effects (add prcp)'
#----------------------------------
m4 <- 'temp~airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS'

#Model 5:
d5 <- 'All two-way interactions (add prcp)'
#------------------------------------------
m5 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS)^2'

#Model 6:
d6 <- 'All two-way interactions (prcp Lag +1)'
#---------------------------------------------
m6 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLagged1LS)^2'

#Model 7:
d7 <- 'All two-way interactions (prcp Lag +2)'
#---------------------------------------------
m7 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLagged2LS)^2'


#Model 8:
d8 <- 'All two-way interactions (prcp Lag +3)'
#---------------------------------------------------------------------
m8 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLagged3LS)^2'


#Model 9:
d9 <- 'All two-way interactions (prcp 5-day mean)'
#-------------------------------------------------
m9 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcp5DayLS)^2'

#Model 10:
d10 <- 'All two-way interactions (prcp  + prcpLags 1, 2, & 3)'
#------------------------------------------------------
m10 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS + prcpLagged1LS + prcpLagged2LS + prcpLagged3LS)^2'

#Model 11:
d11 <- 'All two-way interactions (prcp + prcp Lag +1)'
#---------------------------------------------
m11 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS + prcpLagged1LS)^2'

#Model 12:
d12 <- 'All two-way interactions (prcp + prcp Lag +2)'
#---------------------------------------------
m12 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS  + prcpLagged2LS)^2'

#Model 13:
d13 <- 'All two-way interactions (prcp + prcp Lag +3)'
#---------------------------------------------------------------------
m13 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS + prcpLagged3LS)^2'

#Model 14:
d14 <- 'All two-way interactions (prcp + prcp 5-day mean)'
#-------------------------------------------------
m14 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS + prcp5DayLS)^2'

#Model 15:
d15 <- 'All two-way interactions (all prcp terms)'
#------------------------------------------------------
m15 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS  + prcpLagged1LS + prcpLagged2LS + prcpLagged3LS + prcp5DayLS)^2'

segModels             <- list(m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11, m12, m13, m14, m15)
segModelsDescriptions <- list(d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12, d13, d14, d15)
```

# If you already know the model you want to run then run this code with the model entered. Otherwise skip to next section to compare models.
```{r Already know model}

segModels             <- list(m10)
segModelsDescriptions <- list(d10)

```

```{r Run and validate models}
#=================================================================================================================
#                                                    Segment 2
#=================================================================================================================
#Validate for a random subset of sites:

randomSitesS2 <- sample(unique(et2$site), size = validateFrac*length(unique(et2$site)))
valData2    <- et2[et2$site %in% randomSitesS2,]

mS2  <- list()
vmS2 <- list()

for (i in 1:length(segModels)){
  print(i/length(segModels))  
  
  mS2[[i]] <- lm(as.formula(segModels[[i]]), data=et2)
  
  if (validate){
    vmS2[[i]] <- validateModel( mS2[[i]], et2, valData2 )
    if(createBiasMaps) {makeBiasMap(vmS2[[i]])}
  }
}

#-------------------
#Compare the models:
#-------------------
if(validate){
  for ( i in 1:length(vmS2) ){   
      a <- data.frame(vmS2[[i]]$means, extractAIC(mS2[[i]])[1], extractAIC(mS2[[i]])[2], summary(mS2[[i]])$sigma, segModelsDescriptions[[i]])

      if( i == 1 ) { modelMetricsS2 <- a} else ( modelMetricsS2 <- rbind(modelMetricsS2, a))
  }# for loop
  
} else ( for ( i in 1:length(mS2) ){   
      a <- data.frame(extractAIC(mS2[[i]])[1], extractAIC(mS2[[i]])[2], summary(mS2[[i]])$sigma, segModelsDescriptions[[i]])

      if( i == 1 ) { modelMetricsS2 <- a} else ( modelMetricsS2 <- rbind(modelMetricsS2, a))
  }# for loop
  
)

# Label comparison table
if(validate) {colnames(modelMetricsS2) <- c('meanRMSE', 'meanBiasMean', 'meanBiasSD', 'AIC.df', 'AIC', 'Sigma', 'Model Description')} else (colnames(modelMetricsS2) <- c('AIC.df', 'AIC', 'Sigma', 'Model Description'))
rownames(modelMetricsS2) <-   paste0( 'm', seq(from =1, to = length(mS2), by = 1), 'S2')  

modelMetricsS2

#=================================================================================================================
#                                                    Segment 3
#=================================================================================================================
#Validate for a random subset of sites:

randomSitesS3 <- sample(unique(et3$site), size = validateFrac*length(unique(et3$site)))
valData3    <- et3[et3$site %in% randomSitesS3,]

mS3  <- list()
vmS3 <- list()

for (i in 1:length(segModels)){
  print(i/length(segModels))
  
  mS3[[i]] <- lm(as.formula(segModels[[i]]), data=et3)
  
  if (validate){
    vmS3[[i]] <- validateModel( mS3[[i]], et3, valData3 )
    if(createBiasMaps) {makeBiasMap(vmS3[[i]])}
  }
}

#-------------------
#Compare the models:
#-------------------
if(validate){
  for ( i in 1:length(vmS3) ){   
      a <- data.frame(vmS3[[i]]$means, extractAIC(mS3[[i]])[1], extractAIC(mS3[[i]])[2], summary(mS3[[i]])$sigma, segModelsDescriptions[[i]])

      if( i == 1 ) { modelMetricsS3 <- a} else ( modelMetricsS3 <- rbind(modelMetricsS3, a))
  }# for loop

} else ( for ( i in 1:length(mS3) ){   
      a <- data.frame(extractAIC(mS3[[i]])[1], extractAIC(mS3[[i]])[2], summary(mS3[[i]])$sigma, segModelsDescriptions[[i]])

      if( i == 1 ) { modelMetricsS3 <- a} else ( modelMetricsS3 <- rbind(modelMetricsS3, a))
  }# for loop
)

# Label comparison table
if(validate) {colnames(modelMetricsS3) <- c('meanRMSE', 'meanBiasMean', 'meanBiasSD', 'AIC.df', 'AIC', 'Sigma', 'Model Description')} else (colnames(modelMetricsS3) <- c('AIC.df', 'AIC', 'Sigma', 'Model Description'))
rownames(modelMetricsS3) <-   paste0( 'm', seq(from =1, to = length(mS3), by = 1), 'S3')  


modelMetricsS3
```

```{r Save model metrics}
#=================================================================================================================
#                                                   Save the model metrics
#=================================================================================================================

# Want to save the metrics but it's a "list" so can't be a .csv for some reason. Savings as .RData for now.
if(validate){
  segmentModelMetrics <- rbind(modelMetricsS2, modelMetricsS3)
  save(segmentModelMetrics, file=paste0(dataOutDir, sourceList,'/segmentModelMetrics.RData' ) )
}
```

```{r Choose models to use and predict stream temp values}
#After model comparison, choose which model to use to predict:

predictionModelNum <- 1

finalModelS2  <- mS2 [[predictionModelNum]]; finalModelS3   <- mS3 [[predictionModelNum]]
if(validate) {finalValModS2 <- vmS2[[predictionModelNum]]; finalValModS3  <- vmS3[[predictionModelNum]]}

# Predict stream temperatures:
et2[,c('pred', 'lowr', 'upr')] <- predict (finalModelS2, newdata=et2, interval = "confidence") #default confidence interval (CI) = 95%
et3[,c('pred', 'lowr', 'upr')] <- predict (finalModelS3, newdata=et3, interval = "confidence") #default confidence interval (CI) = 95%

# Predict air/stream temperature slopes:
et2Slopes <- ddply( et2, .(site), summarize, slopeSeg2=coef(lm(pred ~ airTemp))[2])
et3Slopes <- ddply( et3, .(site), summarize, slopeSeg3=coef(lm(pred ~ airTemp))[2])

siteData <- merge(siteData, et2Slopes, by = 'site', all.x = T, sort = F)
siteData <- merge(siteData, et3Slopes, by = 'site', all.x = T, sort = F)

```

```{r graphs of predicted/obs etc}

# In this section we want to add in coloring by state or agency
#   Use:     geom_point(aes(color = factor(site))) +

#Also: "geom_text(aes(label=site))" for looking at specific sites

# Plot all observed vs predicted data:
gPredObs <- 
  ggplot(et3,aes(pred,temp))+
  geom_point(size=0.5) +
    scale_x_continuous(expression(paste("Predicted water temperature (",degree, "C)", sep = "")))+
    scale_y_continuous(expression(paste("Observed water temperature (",degree, "C)", sep = "")))+
  theme_bw(base_size=20) +
  geom_abline(intercept=0,slope=1,color='white')

ggsave( file=paste0(graphsDir, sourceList,'/predObs.png'), plot=gPredObs, dpi=dpiIn , width=8,height=5, units='in' )

summary(lm(temp~pred,et3))

if(validate){
  
  #Plot the mean bias from the validation:
  gMeanBias <- ggplot( finalValModS2$v, aes(biasMean)) +
    geom_histogram()+
    scale_x_continuous('Mean bias for each site/year') +
    scale_y_continuous('Frequency')
  
  #Plot the std dev of the bias from the validation:
  gSDBias <- ggplot( finalValModS2$v, aes(biasMean,biasSD)) +
    geom_point()+
    scale_x_continuous('Mean bias for each site/year') +
    scale_y_continuous('SD bias for each site/year')
  
  gValStats <- arrangeGrob( gMeanBias, gSDBias, ncol=1 )
  
  ggsave( file=paste0(graphsDir, sourceList,'/modelValidation.png'), plot=gValStats, dpi=dpiIn , width=8,height=5, units='in' )

}
```

Models for breakpoints
----------------------
When adding new models:
   1) Follow the structure of previously defined models and name it: "bpmX"
   2) Assign the description of the model and name it: "bpdX"
   3) Add new models to the model lists ("bdModels" and "bpModelsDescriptions")

```{r regression bp~+...}
#This section models the breakpoints as a function of fixed covariates.

#=================================================================================================================
#                                          Define the structure of the breakpoint models
#=================================================================================================================

#Breakpoint Model 1:
#-------------------
bpd1 <- 'No interactions'
bpm1 <- '~LatitudeS + LongitudeS + ForestS + AgricultureLS +  BasinElevationMS + ReachSlopePCNTS + TotDASqKMLS + WetlandOrWaterS + SurficialCoarseCLS +(1|year)'

#Breakpoint Model 2:
#-------------------
bpd2 <- 'Full interactions'
bpm2 <-  '~ ( LatitudeS + LongitudeS + ForestS + AgricultureLS + BasinElevationMS + ReachSlopePCNTS + TotDASqKMLS + WetlandOrWaterS + SurficialCoarseCLS )^2 + (1|year)'

#Breakpoint Model 3: 
#-------------------
bpd3 <- 'Full interactions. Impoundments added.'
bpm3 <-  '~ ( LatitudeS + LongitudeS + ForestS + AgricultureLS + BasinElevationMS + ReachSlopePCNTS + TotDASqKMLS + WetlandOrWaterS + SurficialCoarseCLS + ImpoundmentsOpenSqKMLS )^2 + (1|year)'

# List all of the models for reference later:
bpModels <- list(bpm1, bpm2, bpm3)
bpModelsDescriptions <- list(bpd1, bpd2, bpd3)

#=================================================================================================================
#                                               Breakpoint Models
#=================================================================================================================

bp1mods <- list()
bp2mods <- list()
bp3mods <- list()

for ( i in 1:length(bpModels)){
  
  # Breakpoint Models
  bp1mods[[i]] <- lmer(as.formula(paste('springBP', bpModels[[i]])), data=siteData)
  bp2mods[[i]] <- lmer(as.formula(paste('summerBP', bpModels[[i]])), data=siteData)
  bp3mods[[i]] <- lmer(as.formula(paste('fallBP'  , bpModels[[i]])), data=siteData)
  
  # Breakpoint Model AICs
  spr <- data.frame(extractAIC(bp1mods[[i]])[1], extractAIC(bp1mods[[i]])[2], bpModelsDescriptions[[i]])
  names(spr) <- c('df', 'AIC', 'Model Description')
  if( i == 1 ) { sprBPModelMetrics <- spr} else ( sprBPModelMetrics <- rbind(sprBPModelMetrics, spr))

  smr <- data.frame(extractAIC(bp2mods[[i]])[1], extractAIC(bp2mods[[i]])[2], bpModelsDescriptions[[i]])
  names(smr) <- c('df', 'AIC', 'Model Description')
  if( i == 1 ) { smrBPModelMetrics <- smr} else ( smrBPModelMetrics <- rbind(smrBPModelMetrics, smr))

  fal <- data.frame(extractAIC(bp3mods[[i]])[1], extractAIC(bp3mods[[i]])[2], bpModelsDescriptions[[i]])
  names(fal) <- c('df', 'AIC', 'Model Description')
  if( i == 1 ) { falBPModelMetrics <- fal} else ( falBPModelMetrics <- rbind(falBPModelMetrics, fal)) 
}

sprBPModelMetrics
smrBPModelMetrics
falBPModelMetrics

#Checked "AIC" vs "extractAIC" on these models and they give the same values.
```

```{r Choose models to use and predict breakpoints}

bpModelNum <- 3

finalModBP1 <- bp1mods[[bpModelNum]]
finalModBP2 <- bp2mods[[bpModelNum]]
finalModBP3 <- bp3mods[[bpModelNum]]

# not sure why need this [allow.new.levels=T] but throws an error otherwise
#may be because year is in the df

#BP1
siteData$bp1Pred <- predict(finalModBP1,newdata=siteData,allow.new.levels=T)
siteData$bp1PredAvgYear <- predict(finalModBP1,newdata=siteData,REform=NA)

#BP2
siteData$bp2Pred <- predict(finalModBP2,newdata=siteData,allow.new.levels=T)
siteData$bp2PredAvgYear <- predict(finalModBP2,newdata=siteData,REform=NA)

#BP3
siteData$bp3Pred <- predict(finalModBP3,newdata=siteData,allow.new.levels=T)
siteData$bp3PredAvgYear <- predict(finalModBP3,newdata=siteData,REform=NA)

#Synchronized Range
siteData$bp1bp3 <- siteData$bp3Pred - siteData$bp1Pred
siteData$bp1bp3AvgYear <- siteData$bp3PredAvgYear - siteData$bp1PredAvgYear

save(siteData,file=paste0(dataOutDir, sourceList,'/siteDataWBPs_', sourceList, '.RData'))
```

```{r Predicted BP graphs}

if ( makePlots ) {

  #Predicted vs observed spring BP:
  #--------------------------------
  gObsPredBP1 <- 
  ggplot(siteData[siteData$springBP>25,], aes(bp1Pred,springBP))+
    geom_point(aes(color = agency))+
    geom_abline(intercept=0,slope=1)+
    scale_x_continuous("Predicted spring breakpoint")+
    scale_y_continuous("Observed spring breakpoint")+
    theme_bw(base_size=20)
  
  ggsave( file=paste0(graphsDir, sourceList,'/gObsPredBP1_.png'), plot=gObsPredBP1, dpi=dpiIn , width=8,height=5, units='in' )
  
  #Predicted vs observed summer BP:
  #--------------------------------
  #need to look into 2008, low observed values
  gObsPredBP2 <- 
  ggplot(siteData[siteData$summerBP>180&siteData$summerBP<240&siteData$year!=2008,], aes(bp2Pred,summerBP))+
    geom_point()+
    geom_abline(intercept=0,slope=1)+
    scale_x_continuous("Predicted summer breakpoint")+
    scale_y_continuous("Observed summer breakpoint")+
    theme_bw(base_size=20) 
  
  ggsave( file=paste0(graphsDir, sourceList,'/gObsPredBP2_.png'), plot=gObsPredBP2, dpi=dpiIn , width=8,height=5, units='in' )
  
  #Predicted vs observed fall BP:
  #------------------------------
  gObsPredBP3 <- 
  ggplot(siteData, aes(bp3Pred,fallBP))+
    geom_point()+
    geom_abline(intercept=0,slope=1)+
    scale_x_continuous("Predicted fall breakpoint")+
    scale_y_continuous("Observed fall breakpoint")+
    theme_bw(base_size=20)#+facet_wrap(~year) 
  
  ggsave( file=paste0(graphsDir, sourceList,'/gObsPredBP3_.png'), plot=gObsPredBP3, dpi=dpiIn , width=8,height=5, units='in' )

}
```








Predict values for selected catchements
1) Predict breakpoints for UpstreamStats
2) Merge bps into Daymet files
3) Identify segements in Daymet files
3) Predict water temp for segments 2,3

```{r Define prediction region, year, covariates used, and catchments.}

#Pick the area you want to predict for. This is done by selection of daymet tiles:
# See Map Here: http://daymet.ornl.gov/sites/default/files/images/Tiles_on_LCC_projection_300dpi_labels.jpg

DaymetTiles <- c(11754, 11755, 11934, 11935, 12114, 12115)

Year <- 2010

#Read in NHD catchments you want to predict for. This should fall within the boundaries of the daymet tiles above.
proj4.NHD  <- "+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs"

catchments <- readShapePoly ( "C:/KPONEIL/USGS/NHDPlusV2/Modified Versions/CTRiverStates_NHDCatchment.shp", proj4string=CRS(proj4.NHD))

features <- catchments$FEATUREID

#Load the observed covariate data:
load(paste0(dataInDir, 'NENY_CovariateData_2014-04-28.RData'))

#Scale the covariates for prediction to match the model inputs:
#--------------------------------------------------------------
#Normal scaling:
UpstreamStats$LatitudeS        <- (UpstreamStats$Latitude        - mean(UpstreamStats$Latitude        ,na.rm=T)) /sd(UpstreamStats$Latitude        ,na.rm=T)
UpstreamStats$LongitudeS       <- (UpstreamStats$Longitude       - mean(UpstreamStats$Longitude       ,na.rm=T)) /sd(UpstreamStats$Longitude       ,na.rm=T)
UpstreamStats$ForestS          <- (UpstreamStats$Forest          - mean(UpstreamStats$Forest          ,na.rm=T)) /sd(UpstreamStats$Forest          ,na.rm=T)
UpstreamStats$BasinElevationMS <- (UpstreamStats$BasinElevationM - mean(UpstreamStats$BasinElevationM ,na.rm=T)) /sd(UpstreamStats$BasinElevationM ,na.rm=T)
UpstreamStats$ReachSlopePCNTS  <- (UpstreamStats$ReachSlopePCNT  - mean(UpstreamStats$ReachSlopePCNT  ,na.rm=T)) /sd(UpstreamStats$ReachSlopePCNT  ,na.rm=T)
UpstreamStats$WetlandOrWaterS  <- (UpstreamStats$WetlandOrWater  - mean(UpstreamStats$WetlandOrWater  ,na.rm=T)) /sd(UpstreamStats$WetlandOrWater  ,na.rm=T)

#Log scaling:
UpstreamStats$AgricultureLS          <- (log(UpstreamStats$Agriculture          + 0.001) - mean(log(UpstreamStats$Agriculture         + 0.001) ,na.rm=T)) / sd(log(UpstreamStats$Agriculture          + 0.001), na.rm=T)
UpstreamStats$TotDASqKMLS            <- (log(UpstreamStats$TotDASqKM            + 0.001) - mean(log(UpstreamStats$TotDASqKM            +0.001) ,na.rm=T)) / sd(log(UpstreamStats$TotDASqKM            + 0.001), na.rm=T)
UpstreamStats$SurficialCoarseCLS     <- (log(UpstreamStats$SurficialCoarseC     + 1    ) - mean(log(UpstreamStats$SurficialCoarseC     + 1   ) ,na.rm=T)) / sd(log(UpstreamStats$SurficialCoarseC     + 1    ), na.rm=T)
UpstreamStats$ImpoundmentsOpenSqKMLS <- (log(UpstreamStats$ImpoundmentsOpenSqKM + 1    ) - mean(log(UpstreamStats$ImpoundmentsOpenSqKM +1    ) ,na.rm=T)) / sd(log(UpstreamStats$ImpoundmentsOpenSqKM + 1    ), na.rm=T)

```

```{r predicted values for select catchments}

#Here "UpstreamStatsCT" has become "predictionStats" and "CTday2010" has become "FullRecord".

#Select the prediction covariates to index:
predictionCovs <- c('FEATUREID', 'LatitudeS','LongitudeS','ForestS', 'AgricultureLS','BasinElevationMS','ReachSlopePCNTS', 'TotDASqKMLS', 'WetlandOrWaterS','SurficialCoarseCLS', 'ImpoundmentsOpenSqKMLS')

predictionStats <- UpstreamStats[ ,names(UpstreamStats) %in% c(predictionCovs, "StreamOrder")]

rm(UpstreamStats, LocalStats)

predictionStats$year <- Year # for bp predictions

# predict bps
#REform=NA uses no REs. Default is to use all REs (year in our case)
predictionStats$bp1 <- predict(finalModBP1,newdata=predictionStats, allow.new.levels = T)
predictionStats$bp2 <- predict(finalModBP2,newdata=predictionStats, allow.new.levels = T)
predictionStats$bp3 <- predict(finalModBP3,newdata=predictionStats, allow.new.levels = T)

predictionStats$bp1bp3 <- predictionStats$bp3 - predictionStats$bp1
##########################

for ( i in 1:length(DaymetTiles)){
  
  print(i)
  
  # Read in daily data for tile:
  # ============================
  setwd("C:/KPONEIL/GitHub/projects/temperatureProject/dataIn/DaymetClimateData")
  
  load(paste0('NHD_DaymetTile_' , DaymetTiles[i], '_', Year, '.RData'))
  
  # Index the prediction data that matches the Daymet tile:
  predictionStatsTile <- predictionStats[predictionStats$FEATUREID %in% FullRecord$FEATUREID, ]

  # Trim the Daymet tile dataframes to the polygon of catchments you are interested in:
  if(exists('features')){predictionStatsTile <- predictionStatsTile[predictionStatsTile$FEATUREID %in% features, ]}
  if(exists('features')){FullRecord <- FullRecord[FullRecord$FEATUREID %in% features,]}
  
  # Renaming:
  names(FullRecord         )[names(FullRecord         ) == 'FEATUREID'] <- 'site'
  names(predictionStatsTile)[names(predictionStatsTile) == 'FEATUREID'] <- 'site' 
    
  # Merge in bps and assign segments. Do here to make FullRecord smaller:
  # =====================================================================
  FullRecord <- merge( x=FullRecord, y=predictionStatsTile, all.x=T, by = c('site', 'year' ) )
  
  #limit FullRecord to between bp1 and bp3 and assign segments
  FullRecord <- FullRecord[FullRecord$dOY > FullRecord$bp1 & 
                           FullRecord$dOY < FullRecord$bp3,]
  
  FullRecord$segment <- ifelse( FullRecord$dOY > FullRecord$bp1 & 
                               FullRecord$dOY <= FullRecord$bp2, 2,
                        ifelse( FullRecord$dOY > FullRecord$bp2 &
                               FullRecord$dOY <= FullRecord$bp3, 3, NA))
  
  
  # Get rid of site/segment combos with all low # of obs:
  # =====================================================
  counts     <- ddply( FullRecord, .(site,segment), summarize, count=length(na.omit(airTemp)))
  FullRecord <- merge( x=FullRecord, y=counts, all.x=T )
  FullRecord <- FullRecord[ FullRecord$count > 3, ]

  # Do the same for prcp. Need to look into why there are NAs
  counts1    <- ddply( FullRecord, .(site,segment), summarize, count1=length(na.omit(prcp)))
  FullRecord <- merge( x=FullRecord, y=counts1, all.x=T )
  FullRecord <- FullRecord[ FullRecord$count1 > 3, ]
  
  
  # Lag select daymet variables: (slow for big datasets)
  # ============================
  FullRecord <- FullRecord[order(FullRecord$site,FullRecord$dOY),] # just to make sure FullRecord is ordered for the slide function
  
  # airTemp
  FullRecord <- slide(FullRecord, Var = "airTemp", GroupVar = "site", slideBy = -1, NewVar='airTempLagged1')
  FullRecord <- slide(FullRecord, Var = "airTemp", GroupVar = "site", slideBy = -2, NewVar='airTempLagged2')

  # prcp
  FullRecord <- slide(FullRecord, Var = "prcp",    GroupVar = "site", slideBy = -1, NewVar='prcpLagged1')
  FullRecord <- slide(FullRecord, Var = "prcp",    GroupVar = "site", slideBy = -2, NewVar='prcpLagged2')
  FullRecord <- slide(FullRecord, Var = "prcp",    GroupVar = "site", slideBy = -3, NewVar='prcpLagged3')
  
  # Calculate 5-day mean of prcp:
  # =============================
  if('prcp5DayLS' %in% variable.names(finalModelS2) | 'prcp5DayLS' %in% variable.names(finalModelS3) ) {
  
    FullRecord$prcp5Day <- NA
    
    loopSites <- unique(FullRecord$site)
    
    window <- 5
    for (i in 1:length(loopSites)){
    
      currSite <- which(FullRecord$site == loopSites[i])
  
      FullRecord$prcp5Day[currSite] <- rollapply(FullRecord$prcp[currSite], width=window, fill=NA, mean, align = 'left')
    
      if(i/100 == round(i/100)) {print(paste0(i/length(loopSites), '% done 5-day averaging prcp'))}
    }
  }
    
  # Log scale the daymet variables:
  # ===============================
  
  # Only scale the variables that made it into the model:
  predLogVars   <- names(FullRecord)[names(FullRecord) %in% logVariables  ]
  predScaleVars <- names(FullRecord)[names(FullRecord) %in% scaleVariables]
  
  # Scale and log the same variables as the observed data:
  logScaledPreds <- loggAndScaleDaymet(FullRecord, predLogVars, predScaleVars)
  
  FullRecord  <- logScaledPreds[[1]]
  FR2 <- logScaledPreds[[2]]
  FR3 <- logScaledPreds[[3]]
  
  # Predict  Summer Tmax and its CI:
  # ================================
  predictionStatsTile <- predictSummerTMax(FR2, FR3, predictionStatsTile)
  
  # Predict daily water temp
  # ========================
  # Note: running the 'if else' script on the full dataframe runs into memory limitations. This is the reason for the split.
  FR2$predTemp <-predict(finalModelS2,FR2)
  FR3$predTemp <- predict(finalModelS3,FR3)
  
  # Join to over-write FullRecord: (Ok because we already pulled data not in segments 2 or 3)
  # ==============================
  FullRecord <- rbind(FR2, FR3) 
  
  # Estimate slopes of air/water for each site:
  # ===========================================
  predictionStatsTile <- predictSlopes(FullRecord, predictionStatsTile)
  
  # Create a master dataframe of all predictions:
  # =============================================
  TempPreds <- predictionStatsTile[,c('site', 'bp1', 'bp2', 'bp3', 'bp1bp3', 'slopeSeg2', 'slopeSeg2CI',  'slopeSeg3', 'slopeSeg3CI', 'summerMax', 'summerMaxCI')]
    
  if ( i == 1 ) {Predictions <- TempPreds}  else (Predictions <- rbind(Predictions, TempPreds))

  
  

  #Creates folders for graphs if they don't exist:
  #-----------------------------------------------
#  subGraphsDir  <- paste0(graphsDir,  sourceList)
# subDataOutDir <- paste0(dataOutDir, sourceList)
  
 
  
  # Save the record:
  # ================

  # Check to see if the output folder exists. Create one if it doesn't.
  if ( i == 1){
  predictionTileFolder <- paste0(dataOutDir, sourceList, '/modelPredictionTimeseries')
  
    if (!file.exists(predictionTileFolder) ){
      dir.create(file.path(predictionTileFolder))
      }
  }

  # Save it:
  save(FullRecord, file=paste0(predictionTileFolder, '/predictionRecord_DaymetTile' , DaymetTiles[i], '_', Year, '.RData'))
}

```

```{r Write out prediction files for ArcGIS}

names(Predictions)[names(Predictions) == 'site'] <- 'FEATUREID'

Predictions <- merge(Predictions, predictionStats[,c('FEATUREID', 'StreamOrder')], by = 'FEATUREID', all.x = T, sort = F)

#Write out prediction files for ArcGIS:
#--------------------------------------
names(Predictions) <- c('FEATUREID', 'SpringBP', 'SummerBP', 'FallBP', 'SyncTemps', 'RiseSlope', 'RiseSloCI', 'FallSlope', 'FallSloCI', 'SummerMaxT', 'SumrMaxTCI', 'StreamOrder')

Predictions <- replace(Predictions, is.na(Predictions), -9999)

write.csv(Predictions, file = paste0(dataOutDir, sourceList, '/BP_FullPredictions', Year, '.csv'), row.names = F )
write.dbf(Predictions, file = paste0(dataOutDir, sourceList, '/BP_FullPredictions', Year, '.dbf'))

```

```{r Write out predictions for headwaters only (stream order 3 or less)}
streamOrderTrim <- Predictions

streamOrderTrim[ streamOrderTrim$StreamOrder > 3 , c('SpringBP', 'SummerBP', 'FallBP', 'SyncTemps', 'RiseSlope', 'RiseSloCI', 'FallSlope', 'FallSloCI', 'SummerMaxT', 'SumrMaxTCI')] <- -9999

write.csv(streamOrderTrim, file = paste0(dataOutDir, sourceList, '/BP_HeadwatersPredictions', Year, '.csv'), row.names = F )
write.dbf(streamOrderTrim, file = paste0(dataOutDir, sourceList, '/BP_HeadwatersPredictions', Year, '.dbf'))
```
