View(test)
?duplicate
?duplicated
what<- test[duplicated(test$date),]
what
test[(which(test$date == "2008-07-19")),]
duplicated(temp.data$date)
temp.data$site
length(duplicated(master.data$date))
rm(list=ls())
library(ggplot2)
library(GGally)
library(gridExtra)
library(reshape2)
library(mgcv)
library(nlme)
library(plyr)
library(segmented)
library(zoo)
library(ggmap)
library(pls)
library(MASS)
library(lme4)
library(DataCombine) # for the slide function
basedir <- 'C:/KPONEIL/USGS/Stream Temperature/'
#basedir <- 'D:/GitHub/'
#source('D:/GitHub/projects/temperatureProject/temperatureModelingFunctions.R')
source(paste0(basedir, 'temperatureProject/temperatureModelingFunctions.R'))
load('C:/KPONEIL/USGS/GIS/Covariate Stats/NENY_CovariateData_2014-01-23.RData')
Coords <- read.csv('C:/KPONEIL/USGS/NHDPlusV2/Modified Versions/NENY_Catch_Centroids.csv')
str(LocalStats)
head(Coords)
test <- Coords[,c('FETUREID', 'Latitude', 'Longitude')]
class(Coords)
test <- Coords[,c('FEATUREID', 'Latitude', 'Longitude')]
head(test)
test <- merge(LocalStats, Coords, by = 'FEATUREID', all.x = T, sort = F)
load('C:/KPONEIL/USGS/GIS/Covariate Stats/NENY_CovariateData_2014-01-23.RData')
Coords <- read.csv('C:/KPONEIL/USGS/NHDPlusV2/Modified Versions/NENY_Catch_Centroids.csv')
Coords <- Coords[,c('FEATUREID', 'Latitude', 'Longitude')]
test <- merge(LocalStats, Coords, by = 'FEATUREID', all.x = T, sort = F)
head(test)
test <- merge(UpstreamStats, Coords, by = 'FEATUREID', all.x = T, sort = F)
which(is.na(test$Latittude))
which(is.na(test$Latitude))
which(is.na(test$Longitude))
rm(list = ls())
load('C:/KPONEIL/USGS/GIS/Covariate Stats/NENY_CovariateData_2014-01-23.RData')
Coords <- read.csv('C:/KPONEIL/USGS/NHDPlusV2/Modified Versions/NENY_Catch_Centroids.csv')
Coords <- Coords[,c('FEATUREID', 'Latitude', 'Longitude')]
LocalStats <- merge(LocalStats, Coords, by = 'FEATUREID', all.x = T, sort = F)
UpstreamStats <- merge(UpstreamStats, Coords, by = 'FEATUREID', all.x = T, sort = F)
which(is.na(UpstreamStats$Latitude))
which(is.na(UpstreamStats$Longitude))
which(is.na(LocalStats$Longitude))
which(is.na(LocalStats$Latitude))
Sys.Date()
save(UpstreamStats, LocalStats, file = paste0('C:/KPONEIL/USGS/GIS/Covariate Stats/NENY_CovariateData_', Sys.Date(), '.RData')
)
rm(list=ls())
#rm(list=setdiff(ls(), c("Catchments","Sites")))
library(sp)
library(rgdal)
library(rgeos)
library(maptools)     # loads sp library too
library(chron)
library(ncdf)
proj4.NHD  <- "+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs"
#Read in NHD Catchments:
Catchments <- readShapePoly ( "C:/KPONEIL/USGS/Stream Temperature/Shapefiles/NewEnglandCatchments.shp", proj4string=CRS(proj4.NHD))
#Catchments <- readShapePoly ( "C:/KPONEIL/USGS/NHDPlusV2/Modified Versions/ChamplainArea_Catchment.shp", proj4string=CRS(proj4.NHD))
#Read in delineation strings:
#load("C:/KPONEIL/USGS/GIS/Covariate Stats/DelineatedCatchments/DelineatedCatchments_NHDPlus_NENY.RData")
#DelineatedCatchmentsMaster <- NENYDelineatedCatchments
#MasterLength <- length(DelineatedCatchmentsMaster)
#Daymet variables you want:
Variables <- c("dayl", "srad", "swe", "tmax", "tmin", "prcp")
#Which year do you want data for:
year <- 2010
length(Catchment$FEATUREID)
length(Catchments$FEATUREID)
length(unique(Catchments$FEATUREID))
for ( i in 1:length(Catchments$FEATUREID)){   #Site Loop
SiteLat <- coordinates(Catchments[i,])[2]
SiteLon <- coordinates(Catchments[i,])[1]
#Index the tile by site location:
Tile <- ifelse( SiteLat > 40 & SiteLat < 42 & SiteLon > -74 & SiteLon < -72, 11754,
ifelse( SiteLat > 40 & SiteLat < 42 & SiteLon > -72 & SiteLon < -70, 11755,
ifelse( SiteLat > 40 & SiteLat < 42 & SiteLon > -70 & SiteLon < -68, 11756,
ifelse( SiteLat > 42 & SiteLat < 44 & SiteLon > -74 & SiteLon < -72, 11934,
ifelse( SiteLat > 42 & SiteLat < 44 & SiteLon > -72 & SiteLon < -70, 11935,
ifelse( SiteLat > 42 & SiteLat < 44 & SiteLon > -70 & SiteLon < -68, 11936, #**********doublecheck
ifelse( SiteLat > 44 & SiteLat < 46 & SiteLon > -74 & SiteLon < -72, 12114,
ifelse( SiteLat > 44 & SiteLat < 46 & SiteLon > -72 & SiteLon < -70, 12115,
ifelse( SiteLat > 44 & SiteLat < 46 & SiteLon > -70 & SiteLon < -68, 12116, #**********doublecheck
ifelse( SiteLat > 44 & SiteLat < 46 & SiteLon > -68 & SiteLon < -66, 12117, #**********doublecheck
ifelse( SiteLat > 46 & SiteLat < 48 & SiteLon > -72 & SiteLon < -70, 12295, #**********doublecheck
ifelse( SiteLat > 46 & SiteLat < 48 & SiteLon > -70 & SiteLon < -68, 12296, #**********doublecheck
ifelse( SiteLat > 46 & SiteLat < 48 & SiteLon > -68 & SiteLon < -66, 12297, #**********doublecheck
print("Tile Error"))))))))))))))
TileRef <- data.frame(Catchments$FEATUREID[i], Tile, SiteLon, SiteLat)
names(TileRef) <- c('FEATUREID', 'Tile', 'Lon', 'Lat' )
if ( i == 1) { Sites <- TileRef}
if ( i > 1)  { Sites <- rbind (Sites, TileRef)}
print(i)
}
dim(Sites)
head(Sites)
length(unique(Sites$FEATUREID))
Sites <- Sites[order(Sites$Tile),]
#LSites <- length(Sites$FEATUREID)
length(unique(Sites$FEATUREID))
Sites$FEATUREID
Tiles <- unique(Sites$Tile)
Tiles
CurSites <- Sites[Sites$Tile %in% Tiles[k],]
k = 1
CurSites <- Sites[Sites$Tile %in% Tiles[k],]
k = 2
CurSites <- Sites[Sites$Tile %in% Tiles[k],]
LCur <- length(CurSites$FEATUREID)
Tiles
10:9
rm(CurSites)
rm(LCur)
Tiles
start.time <- proc.time()[3]
#for (k in 1:length(Tiles)){
for (k in 10:9){
CurSites <- Sites[Sites$Tile %in% Tiles[k],]
LCur <- length(CurSites$FEATUREID)
for (j in 1:length(Variables)){
for ( i in 1:LCur){   #Site Loop
print(paste0("Processing: Site ", i," of ", LCur, "   |   Variable ", j, " of ", length(Variables), "   |   Tile: ", k, " of ", length(Tiles)))
#ifelse ( i == 1, x <- -9999, x <- Sites$Tile[i-1])
if ( i == 1) {
NCDF <- open.ncdf(paste0("C:/KPONEIL/SourceData/Projected/DAYMET/Daily/", Tiles[k], "_", year,"/", Variables[j], ".nc"))    #netcdf
#Dimension limits of each of the variables we'll use:
#----------------------------------------------------
start1 = c(1,1)
latcount <- c(NCDF$var$lat$varsize[1], NCDF$var$lat$varsize[2])
loncount <- c(NCDF$var$lon$varsize[1], NCDF$var$lon$varsize[2])
YDcount  <- NCDF$var$yearday$varsize
start2 = c(1, 1, 1)
varcount = c(NCDF$var$lat$varsize[1], NCDF$var$lat$varsize[2], NCDF$var$yearday$varsize)
#Read in variables:
#------------------
lat = get.var.ncdf ( nc=NCDF, varid="lat",                 start = start1, count = latcount )
lon = get.var.ncdf ( nc=NCDF, varid="lon",                 start = start1, count = loncount )
dOY = get.var.ncdf ( nc=NCDF, varid="yearday",             start = 1,      count = YDcount  )
var = get.var.ncdf ( nc=NCDF, varid= paste0(Variables[j]), start = start2, count = varcount )
close.ncdf(NCDF)
dOY <- dOY + 1  #Daymet doy starts at 0.
TileCoords <- cbind( as.vector(lon), as.vector(lat))
names(TileCoords) <- c('Lon', 'Lat')
TileCoordsDF <- as.data.frame(TileCoords)
}
#Find the closest daymet point to the site:
#------------------------------------------
distances <- spDistsN1(TileCoords, c(CurSites$Lon[i], CurSites$Lat[i]), longlat = TRUE)
MinDist   <- min(distances)
distpos   <- which(distances == MinDist)[1]
VarLon  <- TileCoordsDF[distpos, 1]
VarLat  <- TileCoordsDF[distpos, 2]
position <- which(lat == VarLat & lon == VarLon, arr.in = TRUE)
#Pull data from that point into dataframe:
#-----------------------------------------
temp.var <- data.frame(CurSites$FEATUREID[i], year, dOY, var[position[1], position[2], 1:365])
names(temp.var) <- c("FEATUREID", "year", "dOY", Variables[j])
#If it's the first instance, create a new storage location:
if (i == 1) {all.sites <- temp.var} else {all.sites <- rbind(all.sites, temp.var)}
}
#Join variable records together:
if (j == 1) {FullRecord <- all.sites} else {FullRecord <- merge(FullRecord, all.sites, by = c("FEATUREID", "year", "dOY"), all.x = T, sort = F)}
}
FullRecord$airTemp <- (FullRecord$tmin + FullRecord$tmax)/2
save(FullRecord, file = paste0("C:/KPONEIL/USGS/Stream Temperature/data/temperature/fromKyle/BP_Analysis/BP_Analysis/DaymetClimateData/NHD_DaymetTile_", Tiles[k],"_", year, ".RData"))
rm(all.sites, FullRecord, temp.var, CurSites, LCur)
}
end.time   <- proc.time()[3]
print(paste0((end.time-start.time)/3600, " hours"))
rm(list = ls())
library(reshape)
DaymetTiles <- c(12115, 12116, 11754, 11755, 11756, 11934, 11935, 11936, 12114, 12117, 12295, 12296, 12297)
Year <- 2010
setwd("C:/KPONEIL/USGS/Stream Temperature/data/temperature/fromKyle/BP_Analysis/BP_Analysis/DaymetClimateData")
for ( i in 1:length(DaymetTiles)){
print(i)
load(paste0('NHD_DaymetTile_' , DaymetTiles[i], '_', Year, '.RData'))
FullRecord <- FullRecord[c('FEATUREID', 'dOY', 'prcp')]
if( i == 1) {PRCP <- FullRecord } else ( PRCP <- rbind(PRCP, FullRecord))
rm(FullRecord)
}
load("C:/KPONEIL/USGS/GIS/Covariate Stats/DelineatedCatchments/DelineatedCatchments_NHDPlus_NENY.RData")
DelineatedCatchmentsMaster <- NENYDelineatedCatchments
MasterLength <- length(DelineatedCatchmentsMaster)
for ( i in 1:length(DaymetTiles)){
load(paste0('NHD_DaymetTile_' , DaymetTiles[i], '_', Year, '.RData'))
sites <- unique(FullRecord$FEATUREID)
for( j in 1:length(sites)){
print(paste0( 'Processing... Site: ', j , ' of ', length(sites),'on Tile: ', i, ' of ', length(DaymetTiles)))
featureID <- sites[j]
features <- DelineatedCatchmentsMaster[[which(sapply(c(1:MasterLength),FUN = function(x){DelineatedCatchmentsMaster[[x]][1]==featureID})==TRUE)]]
Basin <- PRCP[which(PRCP$FEATUREID %in% features),]
BasinCast <- cast(Basin, dOY~FEATUREID, value = 'prcp')
if ( length(features) > 1) {MEAN <- rowMeans(BasinCast[,-1])} else(MEAN <- BasinCast[,2])
BasinMean <- data.frame(featureID, BasinCast$dOY, MEAN)
names(BasinMean) <- c('FEATUREID', 'dOY', 'prcp')
if( j ==1 ) { FullPrcp <- BasinMean} else ( FullPrcp <- rbind(FullPrcp, BasinMean))
}
FullRecord <- FullRecord[, -which(names(FullRecord) == 'prcp')]
FullRecord <- merge(FullRecord, FullPrcp, by = c('FEATUREID', 'dOY'), all.x = T, sort = F)
save(FullRecord, file = paste0('C:/KPONEIL/USGS/Stream Temperature/data/temperature/fromKyle/BP_Analysis/BP_Analysis/DaymetClimateData/UpstreamPrcp/NHD_DaymetTile_' , DaymetTiles[i], '_', Year, '.RData'))
}
library(parallel)
BasinCast
features
Basin
length(BasinCast$prcp)
head(BasinCast)
length(BasinCast$dOY)
tail(FullRecord)
features
sites[j-1]
tail(FullPrcp)
j
for( j in 2513:length(sites)){
print(paste0( 'Processing... Site: ', j , ' of ', length(sites),' | Tile: ', i, ' of ', length(DaymetTiles)))
featureID <- sites[j]
features <- DelineatedCatchmentsMaster[[which(sapply(c(1:MasterLength),FUN = function(x){DelineatedCatchmentsMaster[[x]][1]==featureID})==TRUE)]]
Basin <- PRCP[which(PRCP$FEATUREID %in% features),]
BasinCast <- cast(Basin, dOY~FEATUREID, value = 'prcp')
if ( length(BasinCast$dOY) > 365) {MEAN <- rowMeans(BasinCast[,-1])} else(MEAN <- BasinCast[,2])
BasinMean <- data.frame(featureID, BasinCast$dOY, MEAN)
names(BasinMean) <- c('FEATUREID', 'dOY', 'prcp')
if( j ==1 ) { FullPrcp <- BasinMean} else ( FullPrcp <- rbind(FullPrcp, BasinMean))
}
FullRecord <- FullRecord[, -which(names(FullRecord) == 'prcp')]
FullRecord <- merge(FullRecord, FullPrcp, by = c('FEATUREID', 'dOY'), all.x = T, sort = F)
head(FullRecord)
FullRecord$prcp
paste0('C:/KPONEIL/USGS/Stream Temperature/data/temperature/fromKyle/BP_Analysis/BP_Analysis/DaymetClimateData/UpstreamPrcp/NHD_DaymetTile_' , DaymetTiles[i], '_', Year, '.RData')
save(FullRecord, file = paste0('C:/KPONEIL/USGS/Stream Temperature/data/temperature/fromKyle/BP_Analysis/BP_Analysis/DaymetClimateData/UpstreamPrcp/NHD_DaymetTile_' , DaymetTiles[i], '_', Year, '.RData'))
?rowMeans
#for ( i in 1:length(DaymetTiles)){
for ( i in 3:length(DaymetTiles)){
load(paste0('NHD_DaymetTile_' , DaymetTiles[i], '_', Year, '.RData'))
sites <- unique(FullRecord$FEATUREID)
for( j in 1:length(sites)){
print(paste0( 'Processing... Site: ', j , ' of ', length(sites),' | Tile: ', i, ' of ', length(DaymetTiles)))
featureID <- sites[j]
features <- DelineatedCatchmentsMaster[[which(sapply(c(1:MasterLength),FUN = function(x){DelineatedCatchmentsMaster[[x]][1]==featureID})==TRUE)]]
Basin <- PRCP[which(PRCP$FEATUREID %in% features),]
BasinCast <- cast(Basin, dOY~FEATUREID, value = 'prcp')
if ( length(BasinCast$dOY) > 365) {MEAN <- rowMeans(BasinCast[,-1], na.rm = T)} else(MEAN <- BasinCast[,2])
BasinMean <- data.frame(featureID, BasinCast$dOY, MEAN)
names(BasinMean) <- c('FEATUREID', 'dOY', 'prcp')
if( j ==1 ) { FullPrcp <- BasinMean} else ( FullPrcp <- rbind(FullPrcp, BasinMean))
}
FullRecord <- FullRecord[, -which(names(FullRecord) == 'prcp')]
FullRecord <- merge(FullRecord, FullPrcp, by = c('FEATUREID', 'dOY'), all.x = T, sort = F)
save(FullRecord, file = paste0('C:/KPONEIL/USGS/Stream Temperature/data/temperature/fromKyle/BP_Analysis/BP_Analysis/DaymetClimateData/UpstreamPrcp/NHD_DaymetTile_' , DaymetTiles[i], '_', Year, '.RData'))
}
rm(list = ls())
library(reshape)
DaymetTiles <- c(11754, 11755, 11756, 11934, 11935, 11936, 12114, 12117, 12115, 12116, 12295, 12296, 12297)
Year <- 2010
setwd("C:/KPONEIL/USGS/Stream Temperature/data/temperature/fromKyle/BP_Analysis/BP_Analysis/DaymetClimateData")
for ( i in 1:length(DaymetTiles)){
print(i)
load(paste0('NHD_DaymetTile_' , DaymetTiles[i], '_', Year, '.RData'))
FullRecord <- FullRecord[c('FEATUREID', 'dOY', 'prcp')]
if( i == 1) {PRCP <- FullRecord } else ( PRCP <- rbind(PRCP, FullRecord))
rm(FullRecord)
}
?source
source("C:/KPONEIL/USGS/Scripts/R/StreamTemp/IndexDaymetTileByLatLon.R")
source("C:/KPONEIL/USGS/Scripts/R/StreamTemp/IndexDaymetTileByLatLon.R")
fix(IndexDaymetTileByLatLon)
for ( i in 1:length(Catchments$FEATUREID)){   #Site Loop
SiteLat <- coordinates(Catchments[i,])[2]
SiteLon <- coordinates(Catchments[i,])[1]
Tile <- IndexDaymetTileByLatLon(SiteLat,SiteLon)
TileRef <- data.frame(Catchments$FEATUREID[i], Tile, SiteLon, SiteLat)
names(TileRef) <- c('FEATUREID', 'Tile', 'Lon', 'Lat' )
if ( i == 1) { Sites <- TileRef}
if ( i > 1)  { Sites <- rbind (Sites, TileRef)}
print(i)
}
rm(list=ls())
#rm(list=setdiff(ls(), c("Catchments","Sites")))
library(sp)
library(rgdal)
library(rgeos)
library(maptools)     # loads sp library too
library(chron)
library(ncdf)
source("C:/KPONEIL/USGS/Scripts/R/StreamTemp/IndexDaymetTileByLatLon.R")
proj4.NHD  <- "+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs"
#Read in NHD Catchments:
Catchments <- readShapePoly ( "C:/KPONEIL/USGS/Stream Temperature/Shapefiles/NewEnglandCatchments.shp", proj4string=CRS(proj4.NHD))
#Daymet variables you want:
Variables <- c("dayl", "srad", "swe", "tmax", "tmin", "prcp")
#Which year do you want data for:
year <- 2010
for ( i in 1:length(Catchments$FEATUREID)){   #Site Loop
SiteLat <- coordinates(Catchments[i,])[2]
SiteLon <- coordinates(Catchments[i,])[1]
Tile <- IndexDaymetTileByLatLon(SiteLat,SiteLon)
TileRef <- data.frame(Catchments$FEATUREID[i], Tile, SiteLon, SiteLat)
names(TileRef) <- c('FEATUREID', 'Tile', 'Lon', 'Lat' )
if ( i == 1) { Sites <- TileRef}
if ( i > 1)  { Sites <- rbind (Sites, TileRef)}
print(i)
}
head(Sites)
Sites$Tile
baseDir <- 'C:/KPONEIL/'
dataInDir <- paste0(baseDir, 'GitHub/projects/temperatureProject/dataIn')
dataOutDir <- paste0(baseDir, 'GitHub/projects/temperatureProject/dataOut')
graphsDir <- paste0(baseDir, 'GitHub/projects/temperatureProject/graphs')
setwd(dataInDir)
load('StreamTempData_MASTER.RData')
md <- master.data
setwd(dataInDir)
load('StreamTempData_MASTER.RData')
md <- master.data
splitByAg <- unique(md$agency)
for ( i in 1:length(splitByAg)){
masterData <- md[md$agency == splitByAg[i],]
save(masterData, paste0(dataInDir, '/streamTempData_', splitByAg[i] ))
}
i
file = paste0(dataInDir, '/streamTempData_', splitByAg[i] )
paste0(dataInDir, '/streamTempData_', splitByAg[i] )
rm(file)
dim(masterData)
for ( i in 1:length(splitByAg)){
masterData <- md[md$agency == splitByAg[i],]
save(masterData, file = paste0(dataInDir, '/streamTempData_', splitByAg[i] ))
}
for ( i in 1:length(splitByAg)){
masterData <- md[md$agency == splitByAg[i],]
save(masterData, file = paste0(dataInDir, '/streamTempData_', splitByAg[i], .'RData' ))
}
for ( i in 1:length(splitByAg)){
masterData <- md[md$agency == splitByAg[i],]
save(masterData, file = paste0(dataInDir, '/streamTempData_', splitByAg[i], '.RData' ))
}
load('streamTempData_USGS.RData')
head(masterData)
md <- masterData
md$agency <- 'MAUSGS'
md$site <- paste0(md$agency, '_', md$AgencyID)
head(md)
save(masterData, file = paste0(dataInDir, '/streamTempData_MAUSGS.RData' ))
setwd(dataInDir)
#load('StreamTempData_MASTER.RData')
load('BPA_TimeSeriesData_MASTERsites.RData')
md <- master.data
splitByAg <- unique(md$agency)
for ( i in 1:length(splitByAg)){
masterData <- md[md$agency == splitByAg[i],]
save(masterData, file = paste0(dataInDir, '/streamTempDataWithPairedDaymetClimateData_', splitByAg[i], '.RData' ))
}
for ( i in 1:length(splitByAg)){
masterData <- md[md$agency == splitByAg[i],]
save(masterData, file = paste0(dataInDir, '/streamTempSitesObservedClimateData_', splitByAg[i], '.RData' ))
}
load('streamTempSitesObservedClimateData__USGS.RData')
md <- masterData
md$agency <- 'MAUSGS'
md$site <- paste0(md$agency, '_', md$AgencyID)
masterData <- md
save(masterData, file = paste0(dataInDir, '/streamTempSitesObservedClimateData_MAUSGS.RData' ))
#usgs <- which(md$agency == 'USGS')
#md$agency[usgs] <- MAUSGS
rm(c(md, masterData))
rm(list(md, masterData))
rm(list('md', 'masterData'))
rm(md)
rm(masterData)
load('streamTempSitesObservedClimateData_USGS.RData')
md <- masterData
md$agency <- 'MAUSGS'
md$site <- paste0(md$agency, '_', md$AgencyID)
masterData <- md
save(masterData, file = paste0(dataInDir, '/streamTempSitesObservedClimateData_MAUSGS.RData' ))
setwd(dataInDir)
#load('StreamTempData_MASTER.RData')
#load('BPA_TimeSeriesData_MASTERsites.RData')
load('NewCovariateData_MASTERsites.RData')
#md <- master.data
#md <- covariate.data
splitByAg <- unique(md$agency)
for ( i in 1:length(splitByAg)){
masterData <- md[md$agency == splitByAg[i],]
save(masterData, file = paste0(dataInDir, '/covariateData_', splitByAg[i], '.RData' ))
}
rm(list = ls())
baseDir <- 'C:/KPONEIL/'
dataInDir <- paste0(baseDir, 'GitHub/projects/temperatureProject/dataIn')
dataOutDir <- paste0(baseDir, 'GitHub/projects/temperatureProject/dataOut')
graphsDir <- paste0(baseDir, 'GitHub/projects/temperatureProject/graphs')
setwd(dataInDir)
load('StreamTempData_MASTER.RData')
load('NewCovariateData_MASTERsites.RData')
md <- master.data
cd <- covariate.data
splitByAg <- unique(md$agency)
load('StreamTempData_MASTER.RData')
load('NewCovariateData_MASTERsites.RData')
md <- master.data
cd <- covariate.data
splitByAg <- unique(md$agency)
for ( i in 1:length(splitByAg)){
sites <- md$site[md$agency == splitByAg[i],]
covariateData <- cd[cd$site %in% sites, ]
save(covariateData, file = paste0(dataInDir, '/covariateData_', splitByAg[i], '.RData' ))
}
splitByAg[i]
for ( i in 1:length(splitByAg)){
sites <- md$site[md$agency == splitByAg[i]]
covariateData <- cd[cd$site %in% sites, ]
save(covariateData, file = paste0(dataInDir, '/covariateData_', splitByAg[i], '.RData' ))
}
paste0('MA', covariateData$site)
load('covariateData_USGS.RData')
covariateData$site <- paste0('MA', covariateData$site)
head(covariateData)
covariateData$site
save(covariateData, file = paste0(dataInDir, '/covariateData_MAUSGS.RData' ))
load('StreamTempData_USGSMTsites')
load('StreamTempData_USGSMTsites.RData')
head(master.data)
md$agency <- 'MTUSGS'
md$site <- paste0(md$agency, '_', md$AgencyID)
masterData <- md
head(masterData)
unique(masterData$site)
rm(list = ls())
baseDir <- 'C:/KPONEIL/'
dataInDir <- paste0(baseDir, 'GitHub/projects/temperatureProject/dataIn')
dataOutDir <- paste0(baseDir, 'GitHub/projects/temperatureProject/dataOut')
graphsDir <- paste0(baseDir, 'GitHub/projects/temperatureProject/graphs')
setwd(dataInDir)
load('StreamTempData_USGSMTsites.RData')
md <- master.data
md$agency <- 'MTUSGS'
md$site <- paste0(md$agency, '_', md$AgencyID)
masterData <- md
unique(masterData$site)
save(masterData, file = paste0(dataInDir, '/streamTempData_MTUSGS.RData' ))
rm(list=ls())
library(ggplot2)
library(GGally)
library(gridExtra)
library(reshape2)
library(mgcv)
library(nlme)
library(plyr)
library(segmented)
library(zoo)
library(ggmap)
library(pls)
library(MASS)
library(lme4)
library(DataCombine) # for the slide function
#basedir <- 'C:/KPONEIL/USGS/Stream Temperature/'
#basedir <- 'D:/GitHub/'
baseDir <- 'C:/KPONEIL/'
source(paste0(basedir, 'temperatureProject/code/temperatureModelingFunctions.R'))
install.packages("Matrix")
rm(list=ls())
library(ggplot2)
library(GGally)
library(gridExtra)
library(reshape2)
library(mgcv)
library(nlme)
library(plyr)
library(segmented)
library(zoo)
library(ggmap)
library(pls)
library(MASS)
library(lme4)
library(DataCombine) # for the slide function
#basedir <- 'C:/KPONEIL/USGS/Stream Temperature/'
#basedir <- 'D:/GitHub/'
baseDir <- 'C:/KPONEIL/'
source(paste0(baseDir, 'temperatureProject/code/temperatureModelingFunctions.R'))
source(paste0(baseDir, 'GitHub/projects/temperatureProject/code/temperatureModelingFunctions.R'))
install.packages("Matrix")
rm(list=ls())
library(ggplot2)
library(GGally)
library(gridExtra)
library(reshape2)
library(mgcv)
library(nlme)
library(plyr)
library(segmented)
library(zoo)
library(ggmap)
library(pls)
library(MASS)
library(lme4)
library(DataCombine) # for the slide function
baseDir <- 'C:/KPONEIL/'
source(paste0(baseDir, 'GitHub/projects/temperatureProject/code/temperatureModelingFunctions.R'))
remove.packages("Matrix", lib="C:/Program Files/R/R-3.0.2/library")
